{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELwgzDfGFXvr"
   },
   "source": [
    "# **Install**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108595,
     "status": "ok",
     "timestamp": 1746412631330,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "dQ9OIgAs4iWa",
    "outputId": "7937bb6a-e8f6-496b-f76d-1e1aa797911d"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQFlVvECeZsD"
   },
   "source": [
    "# **Imports üì¢**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26140,
     "status": "ok",
     "timestamp": 1746412657483,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Jnc2LOc9eRjD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbpEEA0g1-Be"
   },
   "source": [
    "# **Utils üß∞**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1746412657501,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "2gsWU3wPUVVn"
   },
   "outputs": [],
   "source": [
    "def cal_metrics (all_targets, all_outputs):\n",
    "  from sklearn import metrics\n",
    "  all_targets = all_targets.detach().cpu().numpy()\n",
    "  all_outputs = all_outputs.detach().cpu().numpy()\n",
    "\n",
    "  acc = metrics.accuracy_score(all_targets, all_outputs)\n",
    "  macro_precision = metrics.precision_score(all_targets, all_outputs, average = 'macro', zero_division=1)\n",
    "  macro_recall = metrics.recall_score(all_targets, all_outputs, average = 'macro')\n",
    "  macro_f1 = metrics.f1_score(all_targets, all_outputs, average = 'macro')\n",
    "\n",
    "  return acc, macro_precision, macro_recall, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746412657531,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "G6s889_UrqDT"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1746412657565,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "K_JNWs7q0m1h"
   },
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters())/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1746412657569,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "jDuJ8gpkTQ3h"
   },
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746412657572,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "6EXzDEYJZLAY"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, targets):\n",
    "    # Convert softmax predictions to class labels\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    # Calculate true positives, false positives, and false negatives\n",
    "    true_positives = torch.sum((predicted_labels == 1) & (targets == 1)).item()\n",
    "    false_positives = torch.sum((predicted_labels == 1) & (targets == 0)).item()\n",
    "    false_negatives = torch.sum((predicted_labels == 0) & (targets == 1)).item()\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-7)\n",
    "\n",
    "    # Calculate recall\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-7)\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "    return f1_score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1746412657631,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "QneCLLCwvHqf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def save_confusion_matrix(targets, predicted_labels, classes, save_path):\n",
    "    predicted_labels = torch.argmax(predicted_labels, dim=1)\n",
    "    cm = confusion_matrix(targets.cpu().numpy(), predicted_labels.cpu().numpy())\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize confusion matrix\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # Format and display the confusion matrix values\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, format='png')\n",
    "    plt.close()\n",
    "    # Calculate sensitivity and specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746412657639,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "YlTPJ8tHvKat"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def plot_ROC(targets, predicted_labels, save_path):\n",
    "  # predicted_labels = torch.argmax(predicted_labels, dim=1)\n",
    "  fpr, tpr, _ = metrics.roc_curve(targets.cpu().numpy(),  predicted_labels[:,1].cpu().numpy())\n",
    "\n",
    "  noskill_probabilities = [0 for number in range(len(targets.cpu().numpy()))]\n",
    "  fprno, tprno, _ = metrics.roc_curve(targets.cpu().numpy(),  noskill_probabilities)\n",
    "  #create ROC curve\n",
    "  plt.plot(fprno,tprno,'b--')\n",
    "  plt.plot(fpr,tpr,'r')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.savefig(save_path, format='png')\n",
    "  plt.close()\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY5JMY1Yftsr"
   },
   "source": [
    "# **Device ‚öôÔ∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746412657657,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Ce6_MpTcfrGi",
    "outputId": "a4ebffc2-6961-4a81-a3d2-5c6b1858b258"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9DiYhr9Yj-p"
   },
   "source": [
    "# **Dataset üóÇÔ∏è**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1746413261130,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "4Lg9k06qLd9n"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "task = 'right' # left, right, foot, tongue\n",
    "duration = 2 # second\n",
    "apply_filter = False\n",
    "fl, fh = [0.5, 4] # Delta = [0.5, 4], Theta = [4, 8], Alpha = [8, 13], Beta = [13, 30], Gamma = [30, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uawPxD41yGuK"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1746413261233,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "KOYsgLP6XnoV"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "fs = 250  # Sampling frequency\n",
    "\n",
    "order = 5  # Filter order\n",
    "\n",
    "# Create bandpass filter coefficients\n",
    "nyq = 0.5 * fs\n",
    "low = fl / nyq\n",
    "high = fh / nyq\n",
    "b, a = butter(order, [low, high], btype='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2487,
     "status": "ok",
     "timestamp": 1746413263683,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "ETdFiiOxeDZH",
    "outputId": "45c0d031-3e00-4437-c557-cc3cfdd30b40"
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1,10):\n",
    "  data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "  data_val = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000_val/sub{i}/data_{task}_sub{i}.mat')\n",
    "  if duration == 4:\n",
    "    data1 = data[f'data_{task}'][:,channels,:]\n",
    "    data_val = data_val[f'data_{task}'][:,channels,:]\n",
    "    data = np.concatenate((data1, data_val), axis=0)\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "  if duration == 2:\n",
    "    data1 = data[f'data_{task}'][:,channels,:500]\n",
    "    data2 = data[f'data_{task}'][:,channels,500:1000]\n",
    "    data1_val = data_val[f'data_{task}'][:,channels,:500]\n",
    "    data2_val = data_val[f'data_{task}'][:,channels,500:1000]\n",
    "    data = np.concatenate((data1, data2, data1_val, data2_val), axis=0)\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "  if duration == 1:\n",
    "    data1 = data[f'data_{task}'][:,channels,:250]\n",
    "    data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "    data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "    data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "    data1_val = data_val[f'data_{task}'][:,channels,:250]\n",
    "    data2_val = data_val[f'data_{task}'][:,channels,250:500]\n",
    "    data3_val = data_val[f'data_{task}'][:,channels,500:750]\n",
    "    data4_val = data_val[f'data_{task}'][:,channels,750:1000]\n",
    "    data = np.concatenate((data1, data2, data3, data4, data1_val, data2_val, data3_val, data4_val), axis=0)\n",
    "    # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "df = np.array(df)\n",
    "print(df.shape)\n",
    "num_trial = df.shape[1]\n",
    "num_ch = df.shape[2]\n",
    "num_smaple = df.shape[3]\n",
    "df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "label = np.array(label)\n",
    "label = label.reshape((9*num_trial,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1746413263685,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "nB4eLyaAoNyH"
   },
   "outputs": [],
   "source": [
    "# df = []\n",
    "# for i in range(1,10):\n",
    "#   data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects/sub{i}/data_{task}_sub{i}.mat')\n",
    "#   data = data[f'data_{task}'][:,channels,:250]\n",
    "#   # data = filtfilt(b, a, data) #frequency filter\n",
    "#   label = [i for i in range(1, 10) for _ in range(72)]\n",
    "#   label = np.array(label).reshape((9, 72))\n",
    "#   df.append(data)\n",
    "# df = np.array(df)\n",
    "# print(df.shape)\n",
    "# num_ch = df.shape[2]\n",
    "# num_smaple = df.shape[3]\n",
    "# df = df.reshape((9*72,num_ch,num_smaple))\n",
    "# label = np.array(label)\n",
    "# label = label.reshape((9*72,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1746413263701,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Jj3BEPoEsFJ_",
    "outputId": "0b9c9e09-f75a-4d22-e0d5-4b97a1396ba7"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1746413263703,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "x_Am6D5e5OSB"
   },
   "outputs": [],
   "source": [
    "label = label -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746413263705,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "DJcId30TITbN"
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746413263722,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "TfVjCKwv4oXx",
    "outputId": "18afc86f-f48a-41a8-e6a2-8e43ae316e2b"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746413263731,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "-F0wsnKVFCdw",
    "outputId": "bba9fc34-649b-4fb0-9a29-bc2d9f6feb63"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1746413263818,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "i7O5UGpsJ609"
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "x_train = x_train.unsqueeze(1)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_train = y_train.squeeze()\n",
    "\n",
    "x_valid = torch.FloatTensor(x_valid)\n",
    "x_valid = x_valid.unsqueeze(1)\n",
    "y_valid = torch.LongTensor(y_valid)\n",
    "y_valid = y_valid.squeeze()\n",
    "\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "x_test = x_test.unsqueeze(1)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "mu = x_train.mean(dim=0)\n",
    "std = x_train.std(dim=0)\n",
    "\n",
    "x_train = (x_train - mu) / std\n",
    "x_valid = (x_valid - mu) / std\n",
    "x_test = (x_test - mu) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1746413263836,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "jf5M-CTZyGuM",
    "outputId": "19aaa30c-7379-4ce9-8792-ee2c9b89bd02"
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1746413263852,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "t108vHZnyGuM",
    "outputId": "01ae52a7-72ef-49d3-dcbe-6d3cf9bdeb1a"
   },
   "outputs": [],
   "source": [
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746413263862,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "i7o6b9kcyGuM",
    "outputId": "fe9205f2-b3a8-454d-d0fe-3af67e0cf759"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1746413263878,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "rCzzfTPZyGuN",
    "outputId": "bafa432e-1606-46b8-e80b-a5a9c417d4e5"
   },
   "outputs": [],
   "source": [
    "torch.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpY1h4DZvHmc"
   },
   "source": [
    "## TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746413263891,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "q-1IcLDEqtzp"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aQRTc6n7-hi"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746413263906,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "MWf29MbO79gp"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=130, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1746413263936,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "gBAeQLxJusuD",
    "outputId": "1e0f5923-8bb3-4f23-84a4-f3029061e532"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v0bwJgZ8mvX"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1746412680056,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "2P2DM2onPtM8"
   },
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, num_ch):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1, 64, kernel_size=(num_ch-1, 1), padding=1)\n",
    "#         self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "\n",
    "#         self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "#         self.batchnorm2 = nn.BatchNorm2d(64)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#         self.maxpool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "#         self.batchnorm3 = nn.BatchNorm2d(128)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "\n",
    "#         self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "#         self.batchnorm4 = nn.BatchNorm2d(128)\n",
    "#         self.relu4 = nn.ReLU()\n",
    "\n",
    "#         self.maxpool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "#         self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "#         self.batchnorm5 = nn.BatchNorm2d(256)\n",
    "#         self.relu5 = nn.ReLU()\n",
    "\n",
    "#         self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "#         self.batchnorm6 = nn.BatchNorm2d(256)\n",
    "#         self.relu6 = nn.ReLU()\n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc = nn.Linear(256, 9)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(x.shape)\n",
    "#         x = self.relu1(self.batchnorm1(self.conv1(x)))\n",
    "#         print(x.shape)\n",
    "#         x = self.relu2(self.batchnorm2(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "#         x = self.maxpool1(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu3(self.batchnorm3(self.conv3(x)))\n",
    "#         print(x.shape)\n",
    "#         x = self.relu4(self.batchnorm4(self.conv4(x)))\n",
    "#         print(x.shape)\n",
    "#         x = self.maxpool2(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu5(self.batchnorm5(self.conv5(x)))\n",
    "#         print(x.shape)\n",
    "#         x = self.relu6(self.batchnorm6(self.conv6(x)))\n",
    "#         print(x.shape)\n",
    "#         x = self.avgpool(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.flatten(x)\n",
    "#         print(x.shape)\n",
    "#         print('-------------------------------------------------------------------------------------------------------')\n",
    "#         x = self.fc(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1746412680069,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "yE8Vhpz945aE"
   },
   "outputs": [],
   "source": [
    "def CNN():\n",
    "  network = nn.Sequential(nn.Conv2d(1, 64, kernel_size=(num_ch-1,1), padding=1),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.Conv2d(64, 64, 3, padding=1),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.MaxPool2d(2, 2), # BSx64x16x16\n",
    "\n",
    "                          nn.Conv2d(64, 128, 3, padding=1),\n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.Conv2d(128, 128, 3, padding=1),\n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.MaxPool2d(2,2), # 8x8\n",
    "\n",
    "                          nn.Conv2d(128, 256, 3, padding=1),\n",
    "                          nn.BatchNorm2d(256),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.Conv2d(256, 256, 3, padding=1),\n",
    "                          nn.BatchNorm2d(256),\n",
    "                          nn.ReLU(),\n",
    "                          # BSx256x8x8 -> BSx256x1x1\n",
    "                          nn.AdaptiveAvgPool2d(output_size=(1, 1)), # BS1x1\n",
    "\n",
    "                          nn.Flatten(), # BSx256\n",
    "                          nn.Linear(256, 9)\n",
    "                      )\n",
    "\n",
    "  return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746412680077,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Y0Ob0eTk8rpx"
   },
   "outputs": [],
   "source": [
    "# model = CNN().to(device)\n",
    "\n",
    "# # Print the model architecture\n",
    "# print(model)\n",
    "# num_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_LljZFVAFPA"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1746412680085,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "4W9MVeEqAYiq"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch=None):\n",
    "  model.train()\n",
    "  loss_train = AverageMeter()\n",
    "  acc_train = Accuracy(task=\"multiclass\", num_classes=9).to(device)\n",
    "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    for inputs, targets in tepoch:\n",
    "      if epoch is not None:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      loss.backward(retain_graph=True)\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss_train.update(loss.item())\n",
    "      acc_train(outputs, targets.int())\n",
    "      tepoch.set_postfix(loss=loss_train.avg,\n",
    "                         accuracy=100.*acc_train.compute().item())\n",
    "  return model, loss_train.avg, acc_train.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746412680104,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "XeTHNHUJJcv_"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch_kd(student, teacher, train_loader, loss_fn, optimizer, epoch=None):\n",
    "  student.train()\n",
    "  loss_train = AverageMeter()\n",
    "  acc_train = Accuracy(task=\"multiclass\", num_classes=9).to(device)\n",
    "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    for inputs, targets in tepoch:\n",
    "      if epoch is not None:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = student(inputs)\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "\n",
    "      loss = loss_fn_kd(outputs, targets, teacher_outputs, T=10, alpha=0.6)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss_train.update(loss.item())\n",
    "      acc_train(outputs, targets.int())\n",
    "      tepoch.set_postfix(loss=loss_train.avg,\n",
    "                         accuracy=100.*acc_train.compute().item())\n",
    "  return student, loss_train.avg, acc_train.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1746412680124,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "G0d_4iYRl0j-"
   },
   "outputs": [],
   "source": [
    "def validation(model, test_loader, loss_fn):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    loss_valid = AverageMeter()\n",
    "    acc_valid = Accuracy(task=\"multiclass\", num_classes=9).to(device)\n",
    "\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      loss_valid.update(loss.item())\n",
    "      acc_valid(outputs, targets.int())\n",
    "      outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "      all_targets.append(targets)\n",
    "      all_outputs.append(outputs)\n",
    "\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "  return loss_valid.avg, acc_valid.compute().item(), all_targets, all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfQWm6fDkAtE"
   },
   "source": [
    "# 5-fold (combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpJ3wtyctQJH"
   },
   "source": [
    "### Step 1: check forward path\n",
    "\n",
    "Calculate loss for one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eeweXrNLvnz"
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "487Ls4jDLyc6"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjKJAbyPLyc9"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# chan = [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "task = 'right' # left, right, foot, tongue\n",
    "apply_filter = True\n",
    "time = [4]\n",
    "band = [[30, 100]]\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5027,
     "status": "ok",
     "timestamp": 1728292101576,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "7VLzVAerLzwn",
    "outputId": "5c2fb13f-9d4f-498c-9153-f5d2d8ee1356"
   },
   "outputs": [],
   "source": [
    "for fl, fh in band:\n",
    "  if fl == 'a':\n",
    "    apply_filter = False\n",
    "  else:\n",
    "    # ------------------------------------------------------------------ Train and Validation Data -------------------------------------------------\n",
    "    fs = 250  # Sampling frequency\n",
    "    order = 5  # Filter order\n",
    "    # Create bandpass filter coefficients\n",
    "    nyq = 0.5 * fs\n",
    "    low = fl / nyq\n",
    "    high = fh / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "  for t in time:\n",
    "    df = []\n",
    "    for i in range(1,10):\n",
    "      data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "      data_val = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000_val/sub{i}/data_{task}_sub{i}.mat')\n",
    "      if t == 4:\n",
    "        data1 = data[f'data_{task}'][:,channels,:]\n",
    "        data_val = data_val[f'data_{task}'][:,channels,:]\n",
    "        data = np.concatenate((data1, data_val), axis=0)\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 2:\n",
    "        data1 = data[f'data_{task}'][:,channels,:500]\n",
    "        data2 = data[f'data_{task}'][:,channels,500:1000]\n",
    "        data1_val = data_val[f'data_{task}'][:,channels,:500]\n",
    "        data2_val = data_val[f'data_{task}'][:,channels,500:1000]\n",
    "        data = np.concatenate((data1, data2, data1_val, data2_val), axis=0)\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 1:\n",
    "        data1 = data[f'data_{task}'][:,channels,:250]\n",
    "        data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "        data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "        data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "        data1_val = data_val[f'data_{task}'][:,channels,:250]\n",
    "        data2_val = data_val[f'data_{task}'][:,channels,250:500]\n",
    "        data3_val = data_val[f'data_{task}'][:,channels,500:750]\n",
    "        data4_val = data_val[f'data_{task}'][:,channels,750:1000]\n",
    "        data = np.concatenate((data1, data2, data3, data4, data1_val, data2_val, data3_val, data4_val), axis=0)\n",
    "        # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "    df = np.array(df)\n",
    "    print(df.shape)\n",
    "    num_trial = df.shape[1]\n",
    "    num_ch = df.shape[2]\n",
    "    num_smaple = df.shape[3]\n",
    "    df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "    label = np.array(label)\n",
    "    label = label.reshape((9*num_trial,))\n",
    "    label = label -1\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)\n",
    "\n",
    "    x_train = torch.FloatTensor(x_train)\n",
    "    x_train = x_train.unsqueeze(1)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_train = y_train.squeeze()\n",
    "    x_valid = torch.FloatTensor(x_valid)\n",
    "    x_valid = x_valid.unsqueeze(1)\n",
    "    y_valid = torch.LongTensor(y_valid)\n",
    "    y_valid = y_valid.squeeze()\n",
    "    x_test = torch.FloatTensor(x_test)\n",
    "    x_test = x_test.unsqueeze(1)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    mu = x_train.mean(dim=0)\n",
    "    std = x_train.std(dim=0)\n",
    "    x_train = (x_train - mu) / std\n",
    "    x_valid = (x_valid - mu) / std\n",
    "    x_test = (x_test - mu) / std\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llMUP_D7MHvj"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "loss_fn = nn.MultiMarginLoss()\n",
    "lr = 0.00005\n",
    "wd = 3e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1728292112261,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "QnE4F4GkzzaR",
    "outputId": "b76272d2-f2d7-4ea3-90b4-6526c1e972e0"
   },
   "outputs": [],
   "source": [
    "# model = Multimodal(model1, model2).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "outputs = model(x_batch.to(device))\n",
    "loss = loss_fn(outputs, y_batch.to(device))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 889,
     "status": "ok",
     "timestamp": 1728291908498,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "3m7PCCZMZW7A",
    "outputId": "690cb972-0189-4fa4-98dc-fe113df4df08"
   },
   "outputs": [],
   "source": [
    "outputs.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrHQCv7q7LF_"
   },
   "source": [
    "### Step 2: check backward path\n",
    "\n",
    "Select 5 random batches and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jxz5DXoj61mg"
   },
   "outputs": [],
   "source": [
    "_, mini_train_dataset = random_split(train_dataset, (len(train_dataset)-5000,5000))\n",
    "mini_train_loader = DataLoader(mini_train_dataset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8yz1meO_fUS"
   },
   "outputs": [],
   "source": [
    "# model = RNNModel(nn.LSTM, 1, 16, 1, False, 2).to(device)\n",
    "# model = CNNModel([64, 64], [3, 3], 2).to(device)\n",
    "# model = CNNLSTM(1, 32, 128, 3, 2).to(device)\n",
    "\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0AG7IWGEcdG"
   },
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=wd)\n",
    "loss_fn = nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26125,
     "status": "ok",
     "timestamp": 1728292651437,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "PK-P20hI3snf",
    "outputId": "4f2d53cb-3781-4baf-f1c4-b2c5be3fd959"
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "  model, _, _ = train_one_epoch(model, mini_train_loader, loss_fn, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbmG74_8DumN"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuNumM_aqCI2"
   },
   "source": [
    "## save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "842DC1SUqM47"
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wINxdrEqM48"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWVDrZLGqM48"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# chan = [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "task = 'left' # left, right, foot, tongue\n",
    "apply_filter = True\n",
    "time = [4] #[4, 2]\n",
    "band = [['a', 'a']] #[[0.5, 4], [4, 8], [8, 13], [13, 30], [30, 100], ['a', 'a']]\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 434594,
     "status": "ok",
     "timestamp": 1735678976161,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "SrcX8LrfqM49",
    "outputId": "c63e16ca-7866-4f93-9b8a-dbcd73ae743b"
   },
   "outputs": [],
   "source": [
    "for fl, fh in band:\n",
    "  if fl == 'a':\n",
    "    apply_filter = False\n",
    "  else:\n",
    "    # ------------------------------------------------------------------ Train and Validation Data -------------------------------------------------\n",
    "    fs = 250  # Sampling frequency\n",
    "    order = 5  # Filter order\n",
    "    # Create bandpass filter coefficients\n",
    "    nyq = 0.5 * fs\n",
    "    low = fl / nyq\n",
    "    high = fh / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "  for t in time:\n",
    "    df = []\n",
    "    for i in range(1,10):\n",
    "      data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "      data_val = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000_val/sub{i}/data_{task}_sub{i}.mat')\n",
    "      if t == 4:\n",
    "        data1 = data[f'data_{task}'][:,channels,:]\n",
    "        data_val = data_val[f'data_{task}'][:,channels,:]\n",
    "        data = np.concatenate((data1, data_val), axis=0)\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 2:\n",
    "        data1 = data[f'data_{task}'][:,channels,:500]\n",
    "        data2 = data[f'data_{task}'][:,channels,500:1000]\n",
    "        data1_val = data_val[f'data_{task}'][:,channels,:500]\n",
    "        data2_val = data_val[f'data_{task}'][:,channels,500:1000]\n",
    "        data = np.concatenate((data1, data2, data1_val, data2_val), axis=0)\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 1:\n",
    "        data1 = data[f'data_{task}'][:,channels,:250]\n",
    "        data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "        data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "        data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "        data1_val = data_val[f'data_{task}'][:,channels,:250]\n",
    "        data2_val = data_val[f'data_{task}'][:,channels,250:500]\n",
    "        data3_val = data_val[f'data_{task}'][:,channels,500:750]\n",
    "        data4_val = data_val[f'data_{task}'][:,channels,750:1000]\n",
    "        data = np.concatenate((data1, data2, data3, data4, data1_val, data2_val, data3_val, data4_val), axis=0)\n",
    "        # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "    df = np.array(df)\n",
    "    print(df.shape)\n",
    "    num_trial = df.shape[1]\n",
    "    num_ch = df.shape[2]\n",
    "    num_smaple = df.shape[3]\n",
    "    df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "    label = np.array(label)\n",
    "    label = label.reshape((9*num_trial,))\n",
    "    label = label -1\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)\n",
    "    # print(x_train.shape, x_valid.shape, x_test.shape)\n",
    "    # break\n",
    "    x_train = torch.FloatTensor(x_train)\n",
    "    x_train = x_train.unsqueeze(1)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_train = y_train.squeeze()\n",
    "    x_valid = torch.FloatTensor(x_valid)\n",
    "    x_valid = x_valid.unsqueeze(1)\n",
    "    y_valid = torch.LongTensor(y_valid)\n",
    "    y_valid = y_valid.squeeze()\n",
    "    x_test = torch.FloatTensor(x_test)\n",
    "    x_test = x_test.unsqueeze(1)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    mu = x_train.mean(dim=0)\n",
    "    std = x_train.std(dim=0)\n",
    "    x_train = (x_train - mu) / std\n",
    "    x_valid = (x_valid - mu) / std\n",
    "    x_test = (x_test - mu) / std\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "    # --------------------------------------------------------------- K-Fold cross-validation -------------------------------------------------------\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    all_loss_test_hist = []\n",
    "    all_acc_test_hist = []\n",
    "    all_precision_test_hist = []\n",
    "    all_recall_test_hist = []\n",
    "    all_f1_test_hist = []\n",
    "    all_loss_test_hist_s = []\n",
    "    all_acc_test_hist_s = []\n",
    "    all_precision_test_hist_s = []\n",
    "    all_recall_test_hist_s = []\n",
    "    all_f1_test_hist_s = []\n",
    "    all_targests_test_hist = []\n",
    "    all_outputs_test_hist = []\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(x_train)):\n",
    "      print(f\"Fold {fold+1}, fl = {fl}, t = {t}\")\n",
    "      train_sampler = SubsetRandomSampler(train_idx)\n",
    "      valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "      train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "      valid_loader = DataLoader(train_dataset, sampler=valid_sampler, batch_size=32)\n",
    "      test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "      model = CNN().to(device)\n",
    "      loss_fn = nn.MultiMarginLoss()\n",
    "      lr = 0.00005\n",
    "      wd = 3e-4\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "      best_loss_valid = float('inf')\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model, loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n",
    "\n",
    "        # Validation\n",
    "        loss_valid, acc_valid, _, _ = validation(model, valid_loader, loss_fn)\n",
    "\n",
    "        if loss_valid < best_loss_valid:\n",
    "            path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "            torch.save(model, path + '/model_5_fold.pt')\n",
    "            best_loss_valid = loss_valid\n",
    "            print('Model Saved!')\n",
    "\n",
    "        print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "        print()\n",
    "\n",
    "      model = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt')\n",
    "      final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(model, test_loader, loss_fn)\n",
    "      acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets_test, all_outputs_test)\n",
    "\n",
    "      all_loss_test_hist.append(final_loss_test)\n",
    "      all_acc_test_hist.append(final_acc_test)\n",
    "      all_precision_test_hist.append(macro_precision)\n",
    "      all_recall_test_hist.append(macro_recall)\n",
    "      all_f1_test_hist.append(macro_f1)\n",
    "\n",
    "      #------------------------------------------------------------KD----------------------------------------------------------------------\n",
    "\n",
    "      teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt')\n",
    "      # teacher.eval()\n",
    "      student = CNN().to(device)\n",
    "      lr = 0.00005\n",
    "      wd = 3e-4\n",
    "      optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "      loss_fn = nn.MultiMarginLoss()\n",
    "\n",
    "      best_loss_valid_s = torch.inf\n",
    "      epoch_counter = 0\n",
    "\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                            teacher,\n",
    "                                                            train_loader,\n",
    "                                                            loss_fn_kd,\n",
    "                                                            optimizer,\n",
    "                                                            epoch)\n",
    "        # Validation\n",
    "        loss_valid, acc_valid, _, _ = validation(student,\n",
    "                                                 valid_loader,\n",
    "                                                 loss_fn)\n",
    "\n",
    "        if loss_valid < best_loss_valid_s:\n",
    "          # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "          # torch.save(model, path + '/model_6_fold.pt')\n",
    "          best_loss_valid_s = loss_valid\n",
    "          print('best')\n",
    "\n",
    "        print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "        print()\n",
    "\n",
    "        epoch_counter += 1\n",
    "\n",
    "      # student = torch.load('/gdrive/MyDrive/Motor_Imagery/model_6_fold.pt')\n",
    "      # student.eval()\n",
    "      final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(student, test_loader, loss_fn)\n",
    "      acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets_test, all_outputs_test)\n",
    "\n",
    "      all_loss_test_hist_s.append(final_loss_test)\n",
    "      all_acc_test_hist_s.append(final_acc_test)\n",
    "      all_precision_test_hist_s.append(macro_precision)\n",
    "      all_recall_test_hist_s.append(macro_recall)\n",
    "      all_f1_test_hist_s.append(macro_f1)\n",
    "      all_targests_test_hist.append(all_targets_test)\n",
    "      all_outputs_test_hist.append(all_outputs_test)\n",
    "\n",
    "  # --------------------------------------------Save Results----------------------------------------------------------\n",
    "\n",
    "    a1 = sum(all_loss_test_hist)/3\n",
    "    a2 = sum(all_loss_test_hist_s)/3\n",
    "    b1 = sum(all_acc_test_hist)/3\n",
    "    b2 = sum(all_acc_test_hist_s)/3\n",
    "    c1 = sum(all_precision_test_hist)/3\n",
    "    c2 = sum(all_precision_test_hist_s)/3\n",
    "    d1 = sum(all_recall_test_hist)/3\n",
    "    d2 = sum(all_recall_test_hist_s)/3\n",
    "    e1 = sum(all_f1_test_hist)/3\n",
    "    e2 = sum(all_f1_test_hist_s)/3\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([[e1, d1, c1, b1*100, a1, e2, d2, c2, b2*100, a2]],\n",
    "                      columns=['f1', 'recall', 'precision', 'acc', 'loss', 'f1_s', 'recall_s', 'precision_s', 'acc_s', 'loss_s'])\n",
    "\n",
    "    # # Path to the Excel file\n",
    "    # excel_file_path = '/gdrive/MyDrive/Motor_Imagery/resultsyyyyyyyy.xlsx'\n",
    "\n",
    "    # if os.path.exists(excel_file_path):\n",
    "    #     # If the file exists, read the existing data\n",
    "    #     existing_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "    #     # Append the new data\n",
    "    #     updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    # else:\n",
    "    #     # If the file does not exist, create a new DataFrame\n",
    "    #     updated_df = df\n",
    "\n",
    "    # # Write the updated DataFrame back to the Excel file\n",
    "    # with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "    #     updated_df.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1734390539251,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "I3xxfhrBuTgt",
    "outputId": "d263f48a-3748-465e-f5c0-df016b2703f4"
   },
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1735679016065,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "ls8nxbL1Hsgk",
    "outputId": "9ec9385f-6b97-4cf3-baff-3a051f2791dc"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r95cHK2C97Wq"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcHqRv242O55"
   },
   "outputs": [],
   "source": [
    "all_targets_test_hists = np.concatenate([t.cpu().numpy() for t in all_targests_test_hist])\n",
    "all_outputs_test_hists = np.concatenate([t.cpu().numpy() for t in all_outputs_test_hist])\n",
    "\n",
    "# Now you can create the confusion matrix:\n",
    "cm = confusion_matrix(all_targets_test_hists, all_outputs_test_hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1735679090453,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "zqmYDO34-N7_",
    "outputId": "2aa0f6cc-b7c1-4824-b59f-f5e407af1cee"
   },
   "outputs": [],
   "source": [
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize confusion matrix\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(9)\n",
    "plt.xticks(tick_marks, ['1','2','3','4','5','6','7','8','9'], rotation=45)\n",
    "plt.yticks(tick_marks, ['1','2','3','4','5','6','7','8','9'])\n",
    "fmt = '.2f'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "   plt.text(j, i, format(cm[i, j], fmt), ha='center', va='center',\n",
    "            color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(save_path, format='png')\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIO2T27yp9p3"
   },
   "source": [
    "## save valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ThIGvHanwSOY"
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoNN6hIGwtEd"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8yqPLl8umuE"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# chan = [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "task = 'tongue' # left, right, foot, tongue\n",
    "apply_filter = True\n",
    "time = [4,1]\n",
    "band = [[0.5, 4], [4, 8], [8, 13], [13, 30], [30, 100], ['a', 'a']]\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nNEbE6_UwETg",
    "outputId": "0917b120-de8d-4427-d8f7-531d65c2f48b"
   },
   "outputs": [],
   "source": [
    "for fl, fh in band:\n",
    "  if fl == 'a':\n",
    "    apply_filter = False\n",
    "  else:\n",
    "    fs = 250  # Sampling frequency\n",
    "    order = 5  # Filter order\n",
    "    # Create bandpass filter coefficients\n",
    "    nyq = 0.5 * fs\n",
    "    low = fl / nyq\n",
    "    high = fh / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "  for t in time:\n",
    "    df = []\n",
    "    for i in range(1,10):\n",
    "      data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "      if t == 4:\n",
    "        data = data[f'data_{task}'][:,channels,:]\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 1:\n",
    "        data1 = data[f'data_{task}'][:,channels,:250]\n",
    "        data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "        data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "        data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "        data = np.concatenate((data1, data2, data3, data4), axis=0)\n",
    "        # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "    df = np.array(df)\n",
    "    print(df.shape)\n",
    "    num_trial = df.shape[1]\n",
    "    num_ch = df.shape[2]\n",
    "    num_smaple = df.shape[3]\n",
    "    df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "    label = np.array(label)\n",
    "    label = label.reshape((9*num_trial,))\n",
    "    label = label -1\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)\n",
    "\n",
    "    x_train = torch.FloatTensor(x_train)\n",
    "    x_train = x_train.unsqueeze(1)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_train = y_train.squeeze()\n",
    "    x_valid = torch.FloatTensor(x_valid)\n",
    "    x_valid = x_valid.unsqueeze(1)\n",
    "    y_valid = torch.LongTensor(y_valid)\n",
    "    y_valid = y_valid.squeeze()\n",
    "    x_test = torch.FloatTensor(x_test)\n",
    "    x_test = x_test.unsqueeze(1)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    y_test = y_test.squeeze()\n",
    "    mu = x_train.mean(dim=0)\n",
    "    std = x_train.std(dim=0)\n",
    "    x_train = (x_train - mu) / std\n",
    "    x_valid = (x_valid - mu) / std\n",
    "    x_test = (x_test - mu) / std\n",
    "\n",
    "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "    # KFold cross-validation\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    all_loss_valid_hist = []\n",
    "    all_acc_valid_hist = []\n",
    "    all_precision_valid_hist = []\n",
    "    all_recall_valid_hist = []\n",
    "    all_f1_valid_hist = []\n",
    "    all_loss_valid_hist_s = []\n",
    "    all_acc_valid_hist_s = []\n",
    "    all_precision_valid_hist_s = []\n",
    "    all_recall_valid_hist_s = []\n",
    "    all_f1_valid_hist_s = []\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(x_train)):\n",
    "      print(f\"Fold {fold+1}, fl = {fl}, t = {t}\")\n",
    "      train_sampler = SubsetRandomSampler(train_idx)\n",
    "      valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "      train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "      valid_loader = DataLoader(train_dataset, sampler=valid_sampler, batch_size=32)\n",
    "\n",
    "      model = CNN().to(device)\n",
    "      loss_fn = nn.MultiMarginLoss()\n",
    "      lr = 0.00005\n",
    "      wd = 3e-4\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "      # Histories for this fold\n",
    "      loss_valid_hist = []\n",
    "      acc_valid_hist = []\n",
    "      precision_valid_hist = []\n",
    "      recall_valid_hist = []\n",
    "      f1_valid_hist = []\n",
    "\n",
    "      best_loss_valid = float('inf')\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model, loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n",
    "\n",
    "        # Validation\n",
    "        loss_valid, acc_valid, all_targets, all_outputs = validation(model, valid_loader, loss_fn)\n",
    "\n",
    "        acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "\n",
    "        loss_valid_hist.append(loss_valid)\n",
    "        acc_valid_hist.append(acc_valid)\n",
    "        precision_valid_hist.append(macro_precision)\n",
    "        recall_valid_hist.append(macro_recall)\n",
    "        f1_valid_hist.append(macro_f1)\n",
    "\n",
    "        if loss_valid < best_loss_valid:\n",
    "            path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "            torch.save(model, path + '/model_5_fold.pt')\n",
    "            best_loss_valid = loss_valid\n",
    "            print('Model Saved!')\n",
    "            print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')\n",
    "\n",
    "\n",
    "        print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "        print()\n",
    "\n",
    "      all_loss_valid_hist.append(loss_valid_hist)\n",
    "      all_acc_valid_hist.append(acc_valid_hist)\n",
    "      all_precision_valid_hist.append(precision_valid_hist)\n",
    "      all_recall_valid_hist.append(recall_valid_hist)\n",
    "      all_f1_valid_hist.append(f1_valid_hist)\n",
    "\n",
    "      #------------------------------------------------------------KD----------------------------------------------------------------------\n",
    "\n",
    "      loss_valid_hist = []\n",
    "      acc_valid_hist = []\n",
    "      precision_valid_hist = []\n",
    "      recall_valid_hist = []\n",
    "      f1_valid_hist = []\n",
    "\n",
    "      teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt')\n",
    "      teacher.eval()\n",
    "      student = CNN().to(device)\n",
    "      lr = 0.00005\n",
    "      wd = 3e-4\n",
    "      optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "      loss_fn = nn.MultiMarginLoss()\n",
    "\n",
    "      best_loss_valid_s = torch.inf\n",
    "      epoch_counter = 0\n",
    "\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                            teacher,\n",
    "                                                            train_loader,\n",
    "                                                            loss_fn_kd,\n",
    "                                                            optimizer,\n",
    "                                                            epoch)\n",
    "        # Validation\n",
    "        loss_valid, acc_valid, all_targets, all_outputs = validation(student,\n",
    "                                                                    valid_loader,\n",
    "                                                                    loss_fn)\n",
    "\n",
    "        acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "\n",
    "        loss_valid_hist.append(loss_valid)\n",
    "        acc_valid_hist.append(acc_valid)\n",
    "        precision_valid_hist.append(macro_precision)\n",
    "        recall_valid_hist.append(macro_recall)\n",
    "        f1_valid_hist.append(macro_f1)\n",
    "\n",
    "\n",
    "        if loss_valid < best_loss_valid_s:\n",
    "          # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "          # torch.save(model, path + '/model' + '.pt')\n",
    "          best_loss_valid_s = loss_valid\n",
    "          print('best')\n",
    "          print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')\n",
    "\n",
    "\n",
    "        print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "        print()\n",
    "\n",
    "        epoch_counter += 1\n",
    "\n",
    "      all_loss_valid_hist_s.append(loss_valid_hist)\n",
    "      all_acc_valid_hist_s.append(acc_valid_hist)\n",
    "      all_precision_valid_hist_s.append(precision_valid_hist)\n",
    "      all_recall_valid_hist_s.append(recall_valid_hist)\n",
    "      all_f1_valid_hist_s.append(f1_valid_hist)\n",
    "\n",
    "  # --------------------------------------------Save Results----------------------------------------------------------\n",
    "\n",
    "    a1=0\n",
    "    a2=0\n",
    "    b1=0\n",
    "    b2=0\n",
    "    c1=0\n",
    "    c2=0\n",
    "    d1=0\n",
    "    d2=0\n",
    "    e1=0\n",
    "    e2=0\n",
    "\n",
    "    for i in range(0,5):\n",
    "      a1 = a1 + min(all_loss_valid_hist[i])\n",
    "    a1 = a1/5\n",
    "    for i in range(0,5):\n",
    "      a2 = a2 + min(all_loss_valid_hist_s[i])\n",
    "    a2 = a2/5\n",
    "\n",
    "    for i in range(0,5):\n",
    "      b1 = b1 + max(all_acc_valid_hist[i])\n",
    "    b1 = b1/5\n",
    "    for i in range(0,5):\n",
    "      b2 = b2 + max(all_acc_valid_hist_s[i])\n",
    "    b2 = b2/5\n",
    "\n",
    "    for i in range(0,5):\n",
    "      c1 = c1 + max(all_precision_valid_hist[i])\n",
    "    c1 = c1/5\n",
    "    for i in range(0,5):\n",
    "      c2 = c2 + max(all_precision_valid_hist_s[i])\n",
    "    c2 = c2/5\n",
    "\n",
    "    for i in range(0,5):\n",
    "      d1 = d1 + max(all_recall_valid_hist[i])\n",
    "    d1 = d1/5\n",
    "    for i in range(0,5):\n",
    "      d2 = d2 + max(all_recall_valid_hist_s[i])\n",
    "    d2 = d2/5\n",
    "\n",
    "    for i in range(0,5):\n",
    "      e1 = e1 + max(all_f1_valid_hist[i])\n",
    "    e1 = e1/5\n",
    "    for i in range(0,5):\n",
    "      e2 = e2 + max(all_f1_valid_hist_s[i])\n",
    "    e2 = e2/5\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([[e1, d1, c1, b1*100, a1, e2, d2, c2, b2*100, a2]],\n",
    "                      columns=['f1', 'recall', 'precision', 'acc', 'loss', 'f1_s', 'recall_s', 'precision_s', 'acc_s', 'loss_s'])\n",
    "\n",
    "    # Path to the Excel file\n",
    "    excel_file_path = '/gdrive/MyDrive/Motor_Imagery/results16.xlsx'\n",
    "\n",
    "    if os.path.exists(excel_file_path):\n",
    "        # If the file exists, read the existing data\n",
    "        existing_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "        # Append the new data\n",
    "        updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    else:\n",
    "        # If the file does not exist, create a new DataFrame\n",
    "        updated_df = df\n",
    "\n",
    "    # Write the updated DataFrame back to the Excel file\n",
    "    with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "        updated_df.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxzGERqwxG58"
   },
   "source": [
    "# split 5-fold (solo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbPIXZeQCIT8"
   },
   "source": [
    "## save test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1746412680169,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "IlzZlegWtMPV",
    "outputId": "809d89db-f062-40b1-8f34-d413644a4076"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a dataset from the training data\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1746416115664,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "XqJrbQU7tMDP"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1746419702894,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Ye9THaJfCx0F"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "all_loss_test_hist = []\n",
    "all_acc_test_hist = []\n",
    "all_precision_test_hist = []\n",
    "all_recall_test_hist = []\n",
    "all_f1_test_hist = []\n",
    "\n",
    "all_loss_test_hist_s = []\n",
    "all_acc_test_hist_s = []\n",
    "all_precision_test_hist_s = []\n",
    "all_recall_test_hist_s = []\n",
    "all_f1_test_hist_s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 840011,
     "status": "ok",
     "timestamp": 1746420551234,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "tQjLFINCCKOV",
    "outputId": "0db711fb-1de0-4608-b779-448e368a5e79"
   },
   "outputs": [],
   "source": [
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "    valid_loader = DataLoader(train_dataset, sampler=valid_sampler, batch_size=32)\n",
    "    test_loader = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    loss_fn = nn.MultiMarginLoss()\n",
    "    lr = 0.00005\n",
    "    wd = 3e-4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "\n",
    "    best_loss_valid = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # Train\n",
    "      model, loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n",
    "\n",
    "      # Validation\n",
    "      loss_valid, acc_valid, _, _ = validation(model, valid_loader, loss_fn)\n",
    "\n",
    "      if loss_valid < best_loss_valid:\n",
    "          path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "          torch.save(model, path + '/model_5_fold.pt')\n",
    "          best_loss_valid = loss_valid\n",
    "          print('Model Saved!')\n",
    "\n",
    "      print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "      print()\n",
    "\n",
    "    model = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt', weights_only=False)\n",
    "    final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(model, test_loader, loss_fn)\n",
    "    acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets_test, all_outputs_test)\n",
    "\n",
    "    all_loss_test_hist.append(final_loss_test)\n",
    "    all_acc_test_hist.append(final_acc_test)\n",
    "    all_precision_test_hist.append(macro_precision)\n",
    "    all_recall_test_hist.append(macro_recall)\n",
    "    all_f1_test_hist.append(macro_f1)\n",
    "\n",
    "    #------------------------------------------------------------KD----------------------------------------------------------------------\n",
    "\n",
    "    teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt', weights_only=False)\n",
    "    teacher.eval()\n",
    "    student = CNN().to(device)\n",
    "    lr = 0.00005\n",
    "    wd = 3e-4\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.MultiMarginLoss()\n",
    "\n",
    "    best_loss_valid_s = torch.inf\n",
    "    epoch_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # Train\n",
    "      student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                          teacher,\n",
    "                                                          train_loader,\n",
    "                                                          loss_fn_kd,\n",
    "                                                          optimizer,\n",
    "                                                          epoch)\n",
    "      # Validation\n",
    "      loss_valid, acc_valid, _, _ = validation(student,\n",
    "                                              valid_loader,\n",
    "                                              loss_fn)\n",
    "\n",
    "\n",
    "      if loss_valid < best_loss_valid_s:\n",
    "        # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "        # torch.save(model, path + '/model_5_fold.pt')\n",
    "        best_loss_valid_s = loss_valid\n",
    "        print('best')\n",
    "\n",
    "      print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "      print()\n",
    "\n",
    "      epoch_counter += 1\n",
    "\n",
    "    final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(student, test_loader, loss_fn)\n",
    "    acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets_test, all_outputs_test)\n",
    "\n",
    "    all_loss_test_hist_s.append(final_loss_test)\n",
    "    all_acc_test_hist_s.append(final_acc_test)\n",
    "    all_precision_test_hist_s.append(macro_precision)\n",
    "    all_recall_test_hist_s.append(macro_recall)\n",
    "    all_f1_test_hist_s.append(macro_f1)\n",
    "\n",
    "# --------------------------------------------Save Results----------------------------------------------------------\n",
    "\n",
    "a1 = sum(all_loss_test_hist)/5\n",
    "a2 = sum(all_loss_test_hist_s)/5\n",
    "b1 = sum(all_acc_test_hist)/5\n",
    "b2 = sum(all_acc_test_hist_s)/5\n",
    "c1 = sum(all_precision_test_hist)/5\n",
    "c2 = sum(all_precision_test_hist_s)/5\n",
    "d1 = sum(all_recall_test_hist)/5\n",
    "d2 = sum(all_recall_test_hist_s)/5\n",
    "e1 = sum(all_f1_test_hist)/5\n",
    "e2 = sum(all_f1_test_hist_s)/5\n",
    "\n",
    "df = pd.DataFrame([[e1, d1, c1, b1*100, a1, e2, d2, c2, b2*100, a2]],\n",
    "                  columns=['f1', 'recall', 'precision', 'acc', 'loss', 'f1_s', 'recall_s', 'precision_s', 'acc_s', 'loss_s'])\n",
    "\n",
    "# # Path to the Excel file\n",
    "# excel_file_path = '/gdrive/MyDrive/Motor_Imagery/results5.xlsx'\n",
    "\n",
    "# if os.path.exists(excel_file_path):\n",
    "#     # If the file exists, read the existing data\n",
    "#     existing_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "#     # Append the new data\n",
    "#     updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "# else:\n",
    "#     # If the file does not exist, create a new DataFrame\n",
    "#     updated_df = df\n",
    "\n",
    "# # Write the updated DataFrame back to the Excel file\n",
    "# with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "#     updated_df.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1746420899998,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "PcbY5CcDIN2q",
    "outputId": "e3cfc64e-b693-4ff3-afb0-3aa2be259d55"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1746417180108,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "coms7n1jTCU0"
   },
   "outputs": [],
   "source": [
    "final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(student, test_loader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1746417180977,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "KEdE7do1THqT",
    "outputId": "8c3065eb-a3ba-4ee4-9f43-888ee6c9652a"
   },
   "outputs": [],
   "source": [
    "final_acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdCZLlEtCBkG"
   },
   "source": [
    "## save valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1721336567456,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "lh3icMu6tRUW",
    "outputId": "a49609e6-443a-4bab-9070-1bd20036d2f4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "\n",
    "# Convert your data into PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create a dataset from the training data\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au-w1b8xtRMl"
   },
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Histories for all folds\n",
    "all_loss_valid_hist = []\n",
    "all_acc_valid_hist = []\n",
    "all_precision_valid_hist = []\n",
    "all_recall_valid_hist = []\n",
    "all_f1_valid_hist = []\n",
    "\n",
    "all_loss_valid_hist_s = []\n",
    "all_acc_valid_hist_s = []\n",
    "all_precision_valid_hist_s = []\n",
    "all_recall_valid_hist_s = []\n",
    "all_f1_valid_hist_s = []\n",
    "\n",
    "all_loss_test_hist = []\n",
    "all_acc_test_hist = []\n",
    "\n",
    "# Initialize best loss to a large number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L7fC4FytRD9"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 577469,
     "status": "ok",
     "timestamp": 1721337170442,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "hJkFzgb9kqUT",
    "outputId": "9a2fe404-0e8d-4775-f748-6cb0b90cf420"
   },
   "outputs": [],
   "source": [
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"Fold {fold+1}\")\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "    valid_loader = DataLoader(train_dataset, sampler=valid_sampler, batch_size=32)\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    loss_fn = nn.MultiMarginLoss()\n",
    "    lr = 0.00005\n",
    "    wd = 3e-4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    # Histories for this fold\n",
    "    loss_valid_hist = []\n",
    "    acc_valid_hist = []\n",
    "    precision_valid_hist = []\n",
    "    recall_valid_hist = []\n",
    "    f1_valid_hist = []\n",
    "\n",
    "    best_loss_valid = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # Train\n",
    "      model, loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n",
    "\n",
    "      # Validation\n",
    "      loss_valid, acc_valid, all_targets, all_outputs = validation(model, valid_loader, loss_fn)\n",
    "\n",
    "      acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "\n",
    "      loss_valid_hist.append(loss_valid)\n",
    "      acc_valid_hist.append(acc_valid)\n",
    "      precision_valid_hist.append(macro_precision)\n",
    "      recall_valid_hist.append(macro_recall)\n",
    "      f1_valid_hist.append(macro_f1)\n",
    "\n",
    "      if loss_valid < best_loss_valid:\n",
    "          path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "          torch.save(model, path + '/model_5_fold.pt')\n",
    "          best_loss_valid = loss_valid\n",
    "          print('Model Saved!')\n",
    "          print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')\n",
    "\n",
    "\n",
    "      print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "      print()\n",
    "\n",
    "    all_loss_valid_hist.append(loss_valid_hist)\n",
    "    all_acc_valid_hist.append(acc_valid_hist)\n",
    "    all_precision_valid_hist.append(precision_valid_hist)\n",
    "    all_recall_valid_hist.append(recall_valid_hist)\n",
    "    all_f1_valid_hist.append(f1_valid_hist)\n",
    "\n",
    "    #------------------------------------------------------------KD----------------------------------------------------------------------\n",
    "\n",
    "    loss_valid_hist = []\n",
    "    acc_valid_hist = []\n",
    "    precision_valid_hist = []\n",
    "    recall_valid_hist = []\n",
    "    f1_valid_hist = []\n",
    "\n",
    "    teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt')\n",
    "    teacher.eval()\n",
    "    student = CNN().to(device)\n",
    "    lr = 0.00005\n",
    "    wd = 3e-4\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.MultiMarginLoss()\n",
    "\n",
    "    best_loss_valid_s = torch.inf\n",
    "    epoch_counter = 0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # Train\n",
    "      student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                          teacher,\n",
    "                                                          train_loader,\n",
    "                                                          loss_fn_kd,\n",
    "                                                          optimizer,\n",
    "                                                          epoch)\n",
    "      # Validation\n",
    "      loss_valid, acc_valid, all_targets, all_outputs = validation(student,\n",
    "                                                                  valid_loader,\n",
    "                                                                  loss_fn)\n",
    "\n",
    "      acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "\n",
    "      loss_valid_hist.append(loss_valid)\n",
    "      acc_valid_hist.append(acc_valid)\n",
    "      precision_valid_hist.append(macro_precision)\n",
    "      recall_valid_hist.append(macro_recall)\n",
    "      f1_valid_hist.append(macro_f1)\n",
    "\n",
    "\n",
    "      if loss_valid < best_loss_valid_s:\n",
    "        # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "        # torch.save(model, path + '/model_5_fold.pt')\n",
    "        best_loss_valid_s = loss_valid\n",
    "        print('best')\n",
    "        print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')\n",
    "\n",
    "\n",
    "      print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "      print()\n",
    "\n",
    "      epoch_counter += 1\n",
    "\n",
    "    all_loss_valid_hist_s.append(loss_valid_hist)\n",
    "    all_acc_valid_hist_s.append(acc_valid_hist)\n",
    "    all_precision_valid_hist_s.append(precision_valid_hist)\n",
    "    all_recall_valid_hist_s.append(recall_valid_hist)\n",
    "    all_f1_valid_hist_s.append(f1_valid_hist)\n",
    "\n",
    "    test_loader = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "    final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(student, test_loader, loss_fn)\n",
    "    all_loss_test_hist.append(final_loss_test)\n",
    "    all_acc_test_hist.append(final_acc_test)\n",
    "\n",
    "# --------------------------------------------Save Results----------------------------------------------------------\n",
    "\n",
    "a1=0\n",
    "a2=0\n",
    "b1=0\n",
    "b2=0\n",
    "c1=0\n",
    "c2=0\n",
    "d1=0\n",
    "d2=0\n",
    "e1=0\n",
    "e2=0\n",
    "\n",
    "for i in range(0,5):\n",
    "  a1 = a1 + min(all_loss_valid_hist[i])\n",
    "a1 = a1/5\n",
    "for i in range(0,5):\n",
    "  a2 = a2 + min(all_loss_valid_hist_s[i])\n",
    "a2 = a2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  b1 = b1 + max(all_acc_valid_hist[i])\n",
    "b1 = b1/5\n",
    "for i in range(0,5):\n",
    "  b2 = b2 + max(all_acc_valid_hist_s[i])\n",
    "b2 = b2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  c1 = c1 + max(all_precision_valid_hist[i])\n",
    "c1 = c1/5\n",
    "for i in range(0,5):\n",
    "  c2 = c2 + max(all_precision_valid_hist_s[i])\n",
    "c2 = c2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  d1 = d1 + max(all_recall_valid_hist[i])\n",
    "d1 = d1/5\n",
    "for i in range(0,5):\n",
    "  d2 = d2 + max(all_recall_valid_hist_s[i])\n",
    "d2 = d2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  e1 = e1 + max(all_f1_valid_hist[i])\n",
    "e1 = e1/5\n",
    "for i in range(0,5):\n",
    "  e2 = e2 + max(all_f1_valid_hist_s[i])\n",
    "e2 = e2/5\n",
    "\n",
    "\n",
    "df = pd.DataFrame([[e1, d1, c1, b1*100, a1, e2, d2, c2, b2*100, a2]],\n",
    "                  columns=['f1_s', 'recall_s', 'precision_s', 'acc_s', 'loss_s', 'f1', 'recall', 'precision', 'acc', 'loss'])\n",
    "\n",
    "# # Path to the Excel file\n",
    "# excel_file_path = '/gdrive/MyDrive/Motor_Imagery/results5.xlsx'\n",
    "\n",
    "# if os.path.exists(excel_file_path):\n",
    "#     # If the file exists, read the existing data\n",
    "#     existing_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "#     # Append the new data\n",
    "#     updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "# else:\n",
    "#     # If the file does not exist, create a new DataFrame\n",
    "#     updated_df = df\n",
    "\n",
    "# # Write the updated DataFrame back to the Excel file\n",
    "# with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "#     updated_df.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1115,
     "status": "ok",
     "timestamp": 1721158397713,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "ZQV0hdAlnO14",
    "outputId": "4a0c8334-a57a-40f8-c4f2-47870da2464c"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 649,
     "status": "ok",
     "timestamp": 1721337183599,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "xY3qUGPjYJJA",
    "outputId": "c19228da-2e2f-4ea9-bf13-d89b8c9eb7ed"
   },
   "outputs": [],
   "source": [
    "print(b1)\n",
    "print(b2)\n",
    "print(sum(all_acc_test_hist)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1720901252096,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "1wyni3TAnk37",
    "outputId": "3dd780ae-0a74-431f-dac1-f42809e3fc1d"
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "a1=0\n",
    "a2=0\n",
    "b1=0\n",
    "b2=0\n",
    "c1=0\n",
    "c2=0\n",
    "d1=0\n",
    "d2=0\n",
    "e1=0\n",
    "e2=0\n",
    "\n",
    "for i in range(0,5):\n",
    "  a1 = a1 + min(all_loss_valid_hist[i])\n",
    "a1 = a1/5\n",
    "for i in range(0,5):\n",
    "  a2 = a2 + min(all_loss_valid_hist_s[i])\n",
    "a2 = a2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  b1 = b1 + max(all_acc_valid_hist[i])\n",
    "b1 = b1/5\n",
    "for i in range(0,5):\n",
    "  b2 = b2 + max(all_acc_valid_hist_s[i])\n",
    "b2 = b2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  c1 = c1 + max(all_precision_valid_hist[i])\n",
    "c1 = c1/5\n",
    "for i in range(0,5):\n",
    "  c2 = c2 + max(all_precision_valid_hist_s[i])\n",
    "c2 = c2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  d1 = d1 + max(all_recall_valid_hist[i])\n",
    "d1 = d1/5\n",
    "for i in range(0,5):\n",
    "  d2 = d2 + max(all_recall_valid_hist_s[i])\n",
    "d2 = d2/5\n",
    "\n",
    "for i in range(0,5):\n",
    "  e1 = e1 + max(all_f1_valid_hist[i])\n",
    "e1 = e1/5\n",
    "for i in range(0,5):\n",
    "  e2 = e2 + max(all_f1_valid_hist_s[i])\n",
    "e2 = e2/5\n",
    "\n",
    "\n",
    "df = pd.DataFrame([[e1, d1, c1, b1*100, a1, e2, d2, c2, b2*100, a2]],\n",
    "                  columns=['f1_s', 'recall_s', 'precision_s', 'acc_s', 'loss_s', 'f1', 'recall', 'precision', 'acc', 'loss'])\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file_path = '/gdrive/MyDrive/Motor_Imagery/results5.xlsx'\n",
    "\n",
    "if os.path.exists(excel_file_path):\n",
    "    # If the file exists, read the existing data\n",
    "    existing_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "    # Append the new data\n",
    "    updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "else:\n",
    "    # If the file does not exist, create a new DataFrame\n",
    "    updated_df = df\n",
    "\n",
    "# Write the updated DataFrame back to the Excel file\n",
    "with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "    updated_df.to_excel(writer, index=False)\n",
    "print('---------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1721337193918,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "IfKRe_DO53Su",
    "outputId": "9b4422d7-6690-4973-ae08-7484a55148f9"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(TensorDataset(x_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(student, test_loader, loss_fn)\n",
    "print(f'Test: Loss = {final_loss_test:.4}, Acc = {final_acc_test:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9csGqnCvtFAS"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746415107492,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "pCCY2WecCyyg"
   },
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "loss_fn = nn.MultiMarginLoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1746415108305,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "bowjVB5yIXUP"
   },
   "outputs": [],
   "source": [
    "lr = 0.00005\n",
    "wd = 3e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1746415109038,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "FIrBCdHBIeRb"
   },
   "outputs": [],
   "source": [
    "loss_train_hist = []\n",
    "loss_valid_hist = []\n",
    "\n",
    "acc_train_hist = []\n",
    "acc_valid_hist = []\n",
    "acc3_valid_hist = []\n",
    "\n",
    "best_loss_valid = torch.inf\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 138476,
     "status": "ok",
     "timestamp": 1746415248517,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "CAXagB4yvtZd",
    "outputId": "38216c15-c82b-4f62-bee4-2c2216a43699"
   },
   "outputs": [],
   "source": [
    "num_epochs = 70\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # Train\n",
    "  model, loss_train, acc_train = train_one_epoch(model,\n",
    "                                                 train_loader,\n",
    "                                                 loss_fn,\n",
    "                                                 optimizer,\n",
    "                                                 epoch)\n",
    "  # Validation\n",
    "  loss_valid, acc_valid, all_targets, all_outputs = validation(model,\n",
    "                                                              valid_loader,\n",
    "                                                              loss_fn)\n",
    "\n",
    "  loss_train_hist.append(loss_train)\n",
    "  loss_valid_hist.append(loss_valid)\n",
    "\n",
    "  acc_train_hist.append(acc_train)\n",
    "  acc_valid_hist.append(acc_valid)\n",
    "\n",
    "  if loss_valid < best_loss_valid:\n",
    "    path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "    torch.save(model, path + '/model' + '.pt')\n",
    "    best_loss_valid = loss_valid\n",
    "    acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "    print('Model Saved!')\n",
    "    print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')\n",
    "\n",
    "  print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "  print()\n",
    "\n",
    "  epoch_counter += 1\n",
    "\n",
    "loss_test, acc_test, all_targets, all_outputs = validation(model,\n",
    "                                                              test_loader,\n",
    "                                                              loss_fn)\n",
    "print(f'Test: Loss = {loss_test:.4}, Acc = {acc_test:.4}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 84,
     "status": "error",
     "timestamp": 1746413106922,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "wIkoRkyN7hhy",
    "outputId": "4c6334f1-aee4-454d-a494-6383e239d3ac"
   },
   "outputs": [],
   "source": [
    "model = torch.load('/gdrive/MyDrive/Motor_Imagery/model.pt')\n",
    "loss_test, acc_test, all_targets, all_outputs = validation(model,\n",
    "                                           valid_loader,\n",
    "                                           loss_fn)\n",
    "\n",
    "acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "\n",
    "print(f'Valid: Loss = {loss_test:.4}, Acc = {acc_test:.4}')\n",
    "print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_e_mBfhakrqo"
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        outputs = model(inputs.to(device))\n",
    "        preds = F.softmax(outputs, dim=1).argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1721374765172,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "bNKaJB-RynPL",
    "outputId": "d2030214-7dd6-4462-ed27-cd5821eefca6"
   },
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot the confusion matrix as before\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(9), yticklabels=range(9))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAQiLjuLwtPY"
   },
   "source": [
    "# Knowledge distillation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fogr5eaQwyQO"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1721374765173,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "ONKQeoL30ghf",
    "outputId": "f6515d7b-9bd7-45b6-c520-0fad3673bf13"
   },
   "outputs": [],
   "source": [
    "teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model.pt')\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAzBtEap0oP5"
   },
   "outputs": [],
   "source": [
    "student = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cn94aG5C0ghl"
   },
   "outputs": [],
   "source": [
    "lr = 0.00005\n",
    "wd = 3e-4\n",
    "optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.MultiMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V20slQYY0ghl"
   },
   "outputs": [],
   "source": [
    "loss_train_hist = []\n",
    "loss_valid_hist = []\n",
    "\n",
    "acc_train_hist = []\n",
    "acc_valid_hist = []\n",
    "\n",
    "best_loss_valid_s = torch.inf\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78110,
     "status": "ok",
     "timestamp": 1721374843263,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "E3CYH4xO0ghl",
    "outputId": "d1899204-fa2d-4210-f4fe-f90269ecb791"
   },
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  # Train\n",
    "  student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                      teacher,\n",
    "                                                      train_loader,\n",
    "                                                      loss_fn_kd,\n",
    "                                                      optimizer,\n",
    "                                                      epoch)\n",
    "  # Validation\n",
    "  loss_valid, acc_valid, all_targets, all_outputs = validation(student,\n",
    "                                                              valid_loader,\n",
    "                                                              loss_fn)\n",
    "\n",
    "  loss_train_hist.append(loss_train)\n",
    "  loss_valid_hist.append(loss_valid)\n",
    "\n",
    "  acc_train_hist.append(acc_train)\n",
    "  acc_valid_hist.append(acc_valid)\n",
    "\n",
    "  if loss_valid < best_loss_valid_s:\n",
    "    # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "    # torch.save(model, path + '/model' + '.pt')\n",
    "    best_loss_valid_s = loss_valid\n",
    "    acc_s, macro_precision_s, macro_recall_s, macro_f1_s = cal_metrics(all_targets, all_outputs)\n",
    "    print('best')\n",
    "    print(f'macro_precision = {macro_precision_s:.4}, macro_recall = {macro_recall_s:.4}, macro_f1 = {macro_f1_s:.4}')\n",
    "\n",
    "  print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "  print()\n",
    "\n",
    "  epoch_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bm2XCErxjVlO"
   },
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1707731771390,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "BPVZg-emjZr1",
    "outputId": "96fad014-6515-450e-ce35-6da44da41b84"
   },
   "outputs": [],
   "source": [
    "print(f'best_acc = {acc:.4}, best_loss = {best_loss_valid:.4}, best_precision = {macro_precision:.4}, best_recall = {macro_recall:.4}, best_f1 = {macro_f1:.4}')\n",
    "print(f'best_acc = {acc_s:.4}, best_loss = {best_loss_valid_s:.4}, best_precision = {macro_precision_s:.4}, best_recall = {macro_recall_s:.4}, best_f1 = {macro_f1_s:.4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK20iNRI3Xxb"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 606,
     "status": "ok",
     "timestamp": 1746413643278,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "KYFzTsdIOkVp",
    "outputId": "7561cbf3-7fa8-40bc-80f1-160f879e86ca"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\n",
    "plt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "ok",
     "timestamp": 1746413643667,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "dUgt-mWsOqhB",
    "outputId": "58c5e0e4-7431-48fd-8900-e8ea341cb8e8"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epoch_counter), acc_train_hist, 'r-', label='Train')\n",
    "plt.plot(range(epoch_counter), acc_valid_hist, 'b-', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Acc')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 5790,
     "status": "ok",
     "timestamp": 1746413649459,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "6Tj9XUDI5-b2",
    "outputId": "0e11e794-1be3-484c-a544-6af26485c50c"
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(range(epoch_counter), loss_train_hist, 'r-', label='Train')\n",
    "plt.plot(range(epoch_counter), loss_valid_hist, 'b-', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Create a 2x1 subplot for accuracy\n",
    "plt.subplot(2, 1, 2)\n",
    "\n",
    "plt.plot(range(epoch_counter), acc_train_hist, 'r-', label='Train')\n",
    "plt.plot(range(epoch_counter), acc_valid_hist, 'b-', label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746413649466,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "bnZUL-8MW8b6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5zctW6jy9Lk"
   },
   "source": [
    "# **Dataset_val üóÇÔ∏è**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwnX3SE4y9Mp"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr0RscBQy9Mq"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "fs = 250  # Sampling frequency\n",
    "f1 = 13  # Lower cutoff frequency\n",
    "f2 = 30  # Upper cutoff frequency\n",
    "order = 5  # Filter order\n",
    "\n",
    "# apply_filter = True\n",
    "# Create bandpass filter coefficients\n",
    "nyq = 0.5 * fs\n",
    "low = f1 / nyq\n",
    "high = f2 / nyq\n",
    "b, a = butter(order, [low, high], btype='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1290,
     "status": "ok",
     "timestamp": 1721466484384,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "9yMUepEOy9Mr",
    "outputId": "b666d6a1-dc08-45b8-a8d3-56301b3b9061"
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1,10):\n",
    "  data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000_val/sub{i}/data_{task}_sub{i}.mat')\n",
    "  if duration == 4:\n",
    "    data = data[f'data_{task}'][:,channels,:]\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "  if duration == 2:\n",
    "    data1 = data[f'data_{task}'][:,channels,:500]\n",
    "    data2 = data[f'data_{task}'][:,channels,500:1000]\n",
    "    data = np.concatenate((data1, data2), axis=0)\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "  if duration == 1:\n",
    "    data1 = data[f'data_{task}'][:,channels,:250]\n",
    "    data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "    data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "    data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "    data = np.concatenate((data1, data2, data3, data4), axis=0)\n",
    "    # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "df = np.array(df)\n",
    "print(df.shape)\n",
    "num_trial = df.shape[1]\n",
    "num_ch = df.shape[2]\n",
    "num_smaple = df.shape[3]\n",
    "df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "label = np.array(label)\n",
    "label = label.reshape((9*num_trial,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QhQLgwTy9Mv"
   },
   "outputs": [],
   "source": [
    "# df = []\n",
    "# for i in range(1,10):\n",
    "#   data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects/sub{i}/data_{task}_sub{i}.mat')\n",
    "#   data = data[f'data_{task}'][:,channels,:250]\n",
    "#   # data = filtfilt(b, a, data) #frequency filter\n",
    "#   label = [i for i in range(1, 10) for _ in range(72)]\n",
    "#   label = np.array(label).reshape((9, 72))\n",
    "#   df.append(data)\n",
    "# df = np.array(df)\n",
    "# print(df.shape)\n",
    "# num_ch = df.shape[2]\n",
    "# num_smaple = df.shape[3]\n",
    "# df = df.reshape((9*72,num_ch,num_smaple))\n",
    "# label = np.array(label)\n",
    "# label = label.reshape((9*72,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1721466484385,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "JW9hvrW3y9M1",
    "outputId": "55088668-d491-4b0f-b27c-920e264e68c8"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1M2zghq0y9M3"
   },
   "outputs": [],
   "source": [
    "label = label -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPnOML2sy9M5"
   },
   "outputs": [],
   "source": [
    "x_test, _, y_test, _ = train_test_split(df, label, test_size=0.1, random_state=23)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1721466484385,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "Hc-DgNUey9M6",
    "outputId": "0de425f6-4df8-47a6-cba7-f5fa0de2e5f7"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1721466484386,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "FLu3BbRdy9M8",
    "outputId": "97528461-9503-4f27-cd52-6ad304686b47"
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxXVtSC8y9M9"
   },
   "outputs": [],
   "source": [
    "x_test = torch.FloatTensor(x_test)\n",
    "x_test = x_test.unsqueeze(1)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "mu = x_train.mean(dim=0)\n",
    "std = x_train.std(dim=0)\n",
    "\n",
    "x_test = (x_test - mu) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1721466484386,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "OMvMkJcYy9NC",
    "outputId": "2aba2bae-63df-414a-9d6e-68a9e5c581af"
   },
   "outputs": [],
   "source": [
    "torch.unique(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwnmMARy9NF"
   },
   "source": [
    "## TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdA4hCXoy9NH"
   },
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qmf-oSeuy9NJ"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VU90wZhoy9NK"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1721466484387,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "ewvs-jXTy9NM",
    "outputId": "0118994d-5da0-47fd-ec0d-4ba74ca2e6cf"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1721466484891,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "RVZsdMxx0ixq",
    "outputId": "3566243b-dcad-4eb5-93a7-9834da767644"
   },
   "outputs": [],
   "source": [
    "# model = torch.load('/gdrive/MyDrive/Motor_Imagery/model.pt')\n",
    "loss_test, acc_test, all_targets, all_outputs = validation(student,\n",
    "                                           test_loader,\n",
    "                                           loss_fn)\n",
    "\n",
    "acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "print(f'Valid: Loss = {loss_test:.4}, Acc = {acc_test:.4}')\n",
    "print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1721466484891,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "HfGOtL240vIQ",
    "outputId": "ca65618f-4bea-4574-e68e-5afb4ef40a11"
   },
   "outputs": [],
   "source": [
    "model(x.to(device))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1721466484891,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "5pIxFfsB0wAG",
    "outputId": "1a7053f6-8117-4ad4-819c-7589c40960bc"
   },
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdSqCI__1O9q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4En2sFilB46f"
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlOgOsERCqrt"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# chan = [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "task = 'tongue' # left, right, foot, tongue\n",
    "apply_filter = True\n",
    "time = [4,1]\n",
    "band = [[0.5, 4], [4, 8], [8, 13], [13, 30], [30, 100], ['a', 'a']]\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1840075,
     "status": "ok",
     "timestamp": 1707847063105,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": -210
    },
    "id": "3Njka42fB7Bh",
    "outputId": "0e33edac-6293-4c76-b956-43caec35e6f6"
   },
   "outputs": [],
   "source": [
    "for fl, fh in band:\n",
    "  if fl == 'a':\n",
    "    apply_filter = False\n",
    "  else:\n",
    "    fs = 250  # Sampling frequency\n",
    "    order = 5  # Filter order\n",
    "    # Create bandpass filter coefficients\n",
    "    nyq = 0.5 * fs\n",
    "    low = fl / nyq\n",
    "    high = fh / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "  for t in time:\n",
    "    df = []\n",
    "    for i in range(1,10):\n",
    "      data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "      if t == 4:\n",
    "        data = data[f'data_{task}'][:,channels,:]\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 1:\n",
    "        data1 = data[f'data_{task}'][:,channels,:250]\n",
    "        data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "        data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "        data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "        data = np.concatenate((data1, data2, data3, data4), axis=0)\n",
    "        # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "    df = np.array(df)\n",
    "    print(df.shape)\n",
    "    num_trial = df.shape[1]\n",
    "    num_ch = df.shape[2]\n",
    "    num_smaple = df.shape[3]\n",
    "    df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "    label = np.array(label)\n",
    "    label = label.reshape((9*num_trial,))\n",
    "    label = label -1\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)\n",
    "\n",
    "    x_train = torch.FloatTensor(x_train)\n",
    "    x_train = x_train.unsqueeze(1)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_train = y_train.squeeze()\n",
    "    x_valid = torch.FloatTensor(x_valid)\n",
    "    x_valid = x_valid.unsqueeze(1)\n",
    "    y_valid = torch.LongTensor(y_valid)\n",
    "    y_valid = y_valid.squeeze()\n",
    "    x_test = torch.FloatTensor(x_test)\n",
    "    x_test = x_test.unsqueeze(1)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    y_test = y_test.squeeze()\n",
    "    mu = x_train.mean(dim=0)\n",
    "    std = x_train.std(dim=0)\n",
    "    x_train = (x_train - mu) / std\n",
    "    x_valid = (x_valid - mu) / std\n",
    "    x_test = (x_test - mu) / std\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=130, shuffle=True)\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    loss_fn = nn.MultiMarginLoss()\n",
    "    lr = 0.00005\n",
    "    wd = 3e-4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_train_hist = []\n",
    "    loss_valid_hist = []\n",
    "\n",
    "    acc_train_hist = []\n",
    "    acc_valid_hist = []\n",
    "    acc3_valid_hist = []\n",
    "\n",
    "    best_loss_valid = torch.inf\n",
    "    epoch_counter = 0\n",
    "\n",
    "    num_epochs = 80\n",
    "    for epoch in range(num_epochs):\n",
    "      # Train\n",
    "      model, loss_train, acc_train = train_one_epoch(model,\n",
    "                                                    train_loader,\n",
    "                                                    loss_fn,\n",
    "                                                    optimizer,\n",
    "                                                    epoch)\n",
    "      # Validation\n",
    "      loss_valid, acc_valid, all_targets, all_outputs = validation(model,\n",
    "                                                                  valid_loader,\n",
    "                                                                  loss_fn)\n",
    "\n",
    "      loss_train_hist.append(loss_train)\n",
    "      loss_valid_hist.append(loss_valid)\n",
    "\n",
    "      acc_train_hist.append(acc_train)\n",
    "      acc_valid_hist.append(acc_valid)\n",
    "\n",
    "      if loss_valid < best_loss_valid:\n",
    "        path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "        torch.save(model, path + '/model' + '.pt')\n",
    "        best_loss_valid = loss_valid\n",
    "        acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets, all_outputs)\n",
    "        print('Model Saved!')\n",
    "        print(f'macro_precision = {macro_precision:.4}, macro_recall = {macro_recall:.4}, macro_f1 = {macro_f1:.4}')\n",
    "\n",
    "      print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "      print()\n",
    "\n",
    "      epoch_counter += 1\n",
    "\n",
    "    teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model.pt')\n",
    "    teacher.eval()\n",
    "    student = CNN().to(device)\n",
    "    lr = 0.00005\n",
    "    wd = 3e-4\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "    # loss_fn = nn.CrossEntropyLoss()\n",
    "    loss_fn = nn.MultiMarginLoss()\n",
    "    loss_train_hist = []\n",
    "    loss_valid_hist = []\n",
    "\n",
    "    acc_train_hist = []\n",
    "    acc_valid_hist = []\n",
    "\n",
    "    best_loss_valid_s = torch.inf\n",
    "    epoch_counter = 0\n",
    "\n",
    "    num_epochs = 80\n",
    "    for epoch in range(num_epochs):\n",
    "      # Train\n",
    "      student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                          teacher,\n",
    "                                                          train_loader,\n",
    "                                                          loss_fn_kd,\n",
    "                                                          optimizer,\n",
    "                                                          epoch)\n",
    "      # Validation\n",
    "      loss_valid, acc_valid, all_targets, all_outputs = validation(student,\n",
    "                                                                  valid_loader,\n",
    "                                                                  loss_fn)\n",
    "\n",
    "      loss_train_hist.append(loss_train)\n",
    "      loss_valid_hist.append(loss_valid)\n",
    "\n",
    "      acc_train_hist.append(acc_train)\n",
    "      acc_valid_hist.append(acc_valid)\n",
    "\n",
    "      if loss_valid < best_loss_valid_s:\n",
    "        # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "        # torch.save(model, path + '/model' + '.pt')\n",
    "        best_loss_valid_s = loss_valid\n",
    "        acc_s, macro_precision_s, macro_recall_s, macro_f1_s = cal_metrics(all_targets, all_outputs)\n",
    "        print('best')\n",
    "        print(f'macro_precision = {macro_precision_s:.4}, macro_recall = {macro_recall_s:.4}, macro_f1 = {macro_f1_s:.4}')\n",
    "\n",
    "      print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "      print()\n",
    "\n",
    "      epoch_counter += 1\n",
    "\n",
    "    # df = pd.DataFrame([[best_loss_valid_s, acc_s, macro_precision_s, macro_recall_s, macro_f1_s, best_loss_valid, acc, macro_precision, macro_recall, macro_f1]])\n",
    "    df = pd.DataFrame([[macro_f1, macro_recall, macro_precision, acc*100, best_loss_valid, macro_f1_s, macro_recall_s, macro_precision_s, acc_s*100, best_loss_valid_s]])\n",
    "    excel_file_path = f'/gdrive/MyDrive/Motor_Imagery/results5.xlsx'\n",
    "    try:\n",
    "        # Load the existing Excel file\n",
    "        existing_wb = load_workbook(excel_file_path)\n",
    "        # Create a Pandas Excel writer using openpyxl\n",
    "        writer = pd.ExcelWriter(excel_file_path, engine='openpyxl')\n",
    "        # Copy the existing sheets\n",
    "        writer.book = existing_wb\n",
    "        # Append the new DataFrame to the existing Excel file\n",
    "        df.to_excel(writer, index=False, header=False, startrow=existing_wb.active.max_row, float_format=\"%.4f\")\n",
    "        # Save the workbook\n",
    "        writer.save()\n",
    "        writer.close()\n",
    "    except FileNotFoundError:\n",
    "        # If the file doesn't exist, create it and write the row\n",
    "        df.to_excel(excel_file_path, header=False, index=False, float_format=\"%.4f\")\n",
    "    print('---------------------------------------------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qL0K8Z91OMxx",
    "ELwgzDfGFXvr",
    "fQFlVvECeZsD",
    "mbpEEA0g1-Be",
    "DY5JMY1Yftsr",
    "G9DiYhr9Yj-p",
    "uawPxD41yGuK",
    "FpY1h4DZvHmc",
    "3aQRTc6n7-hi",
    "7v0bwJgZ8mvX",
    "B_LljZFVAFPA",
    "qfQWm6fDkAtE",
    "lpJ3wtyctQJH",
    "BrHQCv7q7LF_",
    "XuNumM_aqCI2",
    "PIO2T27yp9p3",
    "IxzGERqwxG58",
    "NbPIXZeQCIT8",
    "fdCZLlEtCBkG",
    "9csGqnCvtFAS",
    "mAQiLjuLwtPY",
    "bm2XCErxjVlO",
    "oK20iNRI3Xxb",
    "z5zctW6jy9Lk",
    "EwnX3SE4y9Mp",
    "eQwnmMARy9NF",
    "Qmf-oSeuy9NJ",
    "4En2sFilB46f"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
