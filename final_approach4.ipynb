{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELwgzDfGFXvr"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108595,
     "status": "ok",
     "timestamp": 1746412631330,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "dQ9OIgAs4iWa",
    "outputId": "7937bb6a-e8f6-496b-f76d-1e1aa797911d"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQFlVvECeZsD"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 26140,
     "status": "ok",
     "timestamp": 1746412657483,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Jnc2LOc9eRjD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbpEEA0g1-Be"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1746412657501,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "2gsWU3wPUVVn"
   },
   "outputs": [],
   "source": [
    "def cal_metrics (all_targets, all_outputs):\n",
    "  from sklearn import metrics\n",
    "  all_targets = all_targets.detach().cpu().numpy()\n",
    "  all_outputs = all_outputs.detach().cpu().numpy()\n",
    "\n",
    "  acc = metrics.accuracy_score(all_targets, all_outputs)\n",
    "  macro_precision = metrics.precision_score(all_targets, all_outputs, average = 'macro', zero_division=1)\n",
    "  macro_recall = metrics.recall_score(all_targets, all_outputs, average = 'macro')\n",
    "  macro_f1 = metrics.f1_score(all_targets, all_outputs, average = 'macro')\n",
    "\n",
    "  return acc, macro_precision, macro_recall, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746412657531,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "G6s889_UrqDT"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1746412657565,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "K_JNWs7q0m1h"
   },
   "outputs": [],
   "source": [
    "def num_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters())/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1746412657569,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "jDuJ8gpkTQ3h"
   },
   "outputs": [],
   "source": [
    "def num_trainable_params(model):\n",
    "  nums = sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6\n",
    "  return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746412657572,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "6EXzDEYJZLAY"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, targets):\n",
    "\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    true_positives = torch.sum((predicted_labels == 1) & (targets == 1)).item()\n",
    "    false_positives = torch.sum((predicted_labels == 1) & (targets == 0)).item()\n",
    "    false_negatives = torch.sum((predicted_labels == 0) & (targets == 1)).item()\n",
    "\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-7)\n",
    "\n",
    "\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-7)\n",
    "\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "\n",
    "    return f1_score, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1746412657631,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "QneCLLCwvHqf"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def save_confusion_matrix(targets, predicted_labels, classes, save_path):\n",
    "    predicted_labels = torch.argmax(predicted_labels, dim=1)\n",
    "    cm = confusion_matrix(targets.cpu().numpy(), predicted_labels.cpu().numpy())\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize confusion matrix\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    # Format and display the confusion matrix values\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), ha='center', va='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, format='png')\n",
    "    plt.close()\n",
    "    # Calculate sensitivity and specificity\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    return sensitivity, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1746412657639,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "YlTPJ8tHvKat"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def plot_ROC(targets, predicted_labels, save_path):\n",
    "  # predicted_labels = torch.argmax(predicted_labels, dim=1)\n",
    "  fpr, tpr, _ = metrics.roc_curve(targets.cpu().numpy(),  predicted_labels[:,1].cpu().numpy())\n",
    "\n",
    "  noskill_probabilities = [0 for number in range(len(targets.cpu().numpy()))]\n",
    "  fprno, tprno, _ = metrics.roc_curve(targets.cpu().numpy(),  noskill_probabilities)\n",
    "  #create ROC curve\n",
    "  plt.plot(fprno,tprno,'b--')\n",
    "  plt.plot(fpr,tpr,'r')\n",
    "  plt.ylabel('True Positive Rate')\n",
    "  plt.xlabel('False Positive Rate')\n",
    "  plt.savefig(save_path, format='png')\n",
    "  plt.close()\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DY5JMY1Yftsr"
   },
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746412657657,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Ce6_MpTcfrGi",
    "outputId": "a4ebffc2-6961-4a81-a3d2-5c6b1858b258"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9DiYhr9Yj-p"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1746413261130,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "4Lg9k06qLd9n"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "task = 'right' # left, right, foot, tongue\n",
    "duration = 2 # second\n",
    "apply_filter = False # True, False\n",
    "fl, fh = [0.5, 4] # Delta = [0.5, 4], Theta = [4, 8], Alpha = [8, 13], Beta = [13, 30], Gamma = [30, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uawPxD41yGuK"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "executionInfo": {
     "elapsed": 96,
     "status": "ok",
     "timestamp": 1746413261233,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "KOYsgLP6XnoV"
   },
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "fs = 250  # Sampling frequency\n",
    "\n",
    "order = 5  # Filter order\n",
    "\n",
    "# Create bandpass filter coefficients\n",
    "nyq = 0.5 * fs\n",
    "low = fl / nyq\n",
    "high = fh / nyq\n",
    "b, a = butter(order, [low, high], btype='band')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2487,
     "status": "ok",
     "timestamp": 1746413263683,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "ETdFiiOxeDZH",
    "outputId": "45c0d031-3e00-4437-c557-cc3cfdd30b40"
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for i in range(1,10):\n",
    "  data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "  data_val = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000_val/sub{i}/data_{task}_sub{i}.mat')\n",
    "  if duration == 4:\n",
    "    data1 = data[f'data_{task}'][:,channels,:]\n",
    "    data_val = data_val[f'data_{task}'][:,channels,:]\n",
    "    data = np.concatenate((data1, data_val), axis=0)\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "  if duration == 2:\n",
    "    data1 = data[f'data_{task}'][:,channels,:500]\n",
    "    data2 = data[f'data_{task}'][:,channels,500:1000]\n",
    "    data1_val = data_val[f'data_{task}'][:,channels,:500]\n",
    "    data2_val = data_val[f'data_{task}'][:,channels,500:1000]\n",
    "    data = np.concatenate((data1, data2, data1_val, data2_val), axis=0)\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "  if duration == 1:\n",
    "    data1 = data[f'data_{task}'][:,channels,:250]\n",
    "    data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "    data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "    data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "    data1_val = data_val[f'data_{task}'][:,channels,:250]\n",
    "    data2_val = data_val[f'data_{task}'][:,channels,250:500]\n",
    "    data3_val = data_val[f'data_{task}'][:,channels,500:750]\n",
    "    data4_val = data_val[f'data_{task}'][:,channels,750:1000]\n",
    "    data = np.concatenate((data1, data2, data3, data4, data1_val, data2_val, data3_val, data4_val), axis=0)\n",
    "    # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "    if apply_filter == True:\n",
    "      data = filtfilt(b, a, data) #frequency filter\n",
    "    label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "    label = np.array(label).reshape((9, data.shape[0]))\n",
    "    df.append(data)\n",
    "df = np.array(df)\n",
    "print(df.shape)\n",
    "num_trial = df.shape[1]\n",
    "num_ch = df.shape[2]\n",
    "num_smaple = df.shape[3]\n",
    "df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "label = np.array(label)\n",
    "label = label.reshape((9*num_trial,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1746413263685,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "nB4eLyaAoNyH"
   },
   "outputs": [],
   "source": [
    "# df = []\n",
    "# for i in range(1,10):\n",
    "#   data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects/sub{i}/data_{task}_sub{i}.mat')\n",
    "#   data = data[f'data_{task}'][:,channels,:250]\n",
    "#   # data = filtfilt(b, a, data) #frequency filter\n",
    "#   label = [i for i in range(1, 10) for _ in range(72)]\n",
    "#   label = np.array(label).reshape((9, 72))\n",
    "#   df.append(data)\n",
    "# df = np.array(df)\n",
    "# print(df.shape)\n",
    "# num_ch = df.shape[2]\n",
    "# num_smaple = df.shape[3]\n",
    "# df = df.reshape((9*72,num_ch,num_smaple))\n",
    "# label = np.array(label)\n",
    "# label = label.reshape((9*72,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1746413263701,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "Jj3BEPoEsFJ_",
    "outputId": "0b9c9e09-f75a-4d22-e0d5-4b97a1396ba7"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1746413263703,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "x_Am6D5e5OSB"
   },
   "outputs": [],
   "source": [
    "label = label -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1746413263705,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "DJcId30TITbN"
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746413263722,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "TfVjCKwv4oXx",
    "outputId": "18afc86f-f48a-41a8-e6a2-8e43ae316e2b"
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1746413263731,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "-F0wsnKVFCdw",
    "outputId": "bba9fc34-649b-4fb0-9a29-bc2d9f6feb63"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1746413263818,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "i7O5UGpsJ609"
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "x_train = x_train.unsqueeze(1)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_train = y_train.squeeze()\n",
    "\n",
    "x_valid = torch.FloatTensor(x_valid)\n",
    "x_valid = x_valid.unsqueeze(1)\n",
    "y_valid = torch.LongTensor(y_valid)\n",
    "y_valid = y_valid.squeeze()\n",
    "\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "x_test = x_test.unsqueeze(1)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "y_test = y_test.squeeze()\n",
    "\n",
    "mu = x_train.mean(dim=0)\n",
    "std = x_train.std(dim=0)\n",
    "\n",
    "x_train = (x_train - mu) / std\n",
    "x_valid = (x_valid - mu) / std\n",
    "x_test = (x_test - mu) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1746413263836,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "jf5M-CTZyGuM",
    "outputId": "19aaa30c-7379-4ce9-8792-ee2c9b89bd02"
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1746413263852,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "t108vHZnyGuM",
    "outputId": "01ae52a7-72ef-49d3-dcbe-6d3cf9bdeb1a"
   },
   "outputs": [],
   "source": [
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746413263862,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "i7o6b9kcyGuM",
    "outputId": "fe9205f2-b3a8-454d-d0fe-3af67e0cf759"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1746413263878,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "rCzzfTPZyGuN",
    "outputId": "bafa432e-1606-46b8-e80b-a5a9c417d4e5"
   },
   "outputs": [],
   "source": [
    "torch.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpY1h4DZvHmc"
   },
   "source": [
    "## TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1746413263891,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "q-1IcLDEqtzp"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "test_dataset = TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aQRTc6n7-hi"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746413263906,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "MWf29MbO79gp"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=130, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1746413263936,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "gBAeQLxJusuD",
    "outputId": "1e0f5923-8bb3-4f23-84a4-f3029061e532"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v0bwJgZ8mvX"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1746412680069,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "yE8Vhpz945aE"
   },
   "outputs": [],
   "source": [
    "def CNN():\n",
    "  network = nn.Sequential(nn.Conv2d(1, 64, kernel_size=(num_ch-1,1), padding=1),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.Conv2d(64, 64, 3, padding=1),\n",
    "                          nn.BatchNorm2d(64),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.MaxPool2d(2, 2), # BSx64x16x16\n",
    "\n",
    "                          nn.Conv2d(64, 128, 3, padding=1),\n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.Conv2d(128, 128, 3, padding=1),\n",
    "                          nn.BatchNorm2d(128),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.MaxPool2d(2,2), # 8x8\n",
    "\n",
    "                          nn.Conv2d(128, 256, 3, padding=1),\n",
    "                          nn.BatchNorm2d(256),\n",
    "                          nn.ReLU(),\n",
    "\n",
    "                          nn.Conv2d(256, 256, 3, padding=1),\n",
    "                          nn.BatchNorm2d(256),\n",
    "                          nn.ReLU(),\n",
    "                          # BSx256x8x8 -> BSx256x1x1\n",
    "                          nn.AdaptiveAvgPool2d(output_size=(1, 1)), # BS1x1\n",
    "\n",
    "                          nn.Flatten(), # BSx256\n",
    "                          nn.Linear(256, 9)\n",
    "                      )\n",
    "\n",
    "  return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_LljZFVAFPA"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1746412680085,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "4W9MVeEqAYiq"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch=None):\n",
    "  model.train()\n",
    "  loss_train = AverageMeter()\n",
    "  acc_train = Accuracy(task=\"multiclass\", num_classes=9).to(device)\n",
    "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    for inputs, targets in tepoch:\n",
    "      if epoch is not None:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      loss.backward(retain_graph=True)\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss_train.update(loss.item())\n",
    "      acc_train(outputs, targets.int())\n",
    "      tepoch.set_postfix(loss=loss_train.avg,\n",
    "                         accuracy=100.*acc_train.compute().item())\n",
    "  return model, loss_train.avg, acc_train.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1746412680104,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "XeTHNHUJJcv_"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch_kd(student, teacher, train_loader, loss_fn, optimizer, epoch=None):\n",
    "  student.train()\n",
    "  loss_train = AverageMeter()\n",
    "  acc_train = Accuracy(task=\"multiclass\", num_classes=9).to(device)\n",
    "  with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "    for inputs, targets in tepoch:\n",
    "      if epoch is not None:\n",
    "        tepoch.set_description(f\"Epoch {epoch}\")\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = student(inputs)\n",
    "\n",
    "\n",
    "      with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "\n",
    "      loss = loss_fn_kd(outputs, targets, teacher_outputs, T=10, alpha=0.6)\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      loss_train.update(loss.item())\n",
    "      acc_train(outputs, targets.int())\n",
    "      tepoch.set_postfix(loss=loss_train.avg,\n",
    "                         accuracy=100.*acc_train.compute().item())\n",
    "  return student, loss_train.avg, acc_train.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1746412680124,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "G0d_4iYRl0j-"
   },
   "outputs": [],
   "source": [
    "def validation(model, test_loader, loss_fn):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    loss_valid = AverageMeter()\n",
    "    acc_valid = Accuracy(task=\"multiclass\", num_classes=9).to(device)\n",
    "\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "      inputs = inputs.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      loss_valid.update(loss.item())\n",
    "      acc_valid(outputs, targets.int())\n",
    "      outputs = torch.argmax(outputs, dim=1)\n",
    "\n",
    "      all_targets.append(targets)\n",
    "      all_outputs.append(outputs)\n",
    "\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "    all_outputs = torch.cat(all_outputs, dim=0)\n",
    "\n",
    "  return loss_valid.avg, acc_valid.compute().item(), all_targets, all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfQWm6fDkAtE"
   },
   "source": [
    "# 5-fold (combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "842DC1SUqM47"
   },
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler\n",
    "from scipy.signal import butter, filtfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0wINxdrEqM48"
   },
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T, alpha):\n",
    "  loss = F.kl_div(F.log_softmax(outputs/T, dim=1),\n",
    "                  F.softmax(teacher_outputs/T, dim=1),\n",
    "                  reduction='batchmean') * (alpha * T**2) + \\\n",
    "         F.cross_entropy(outputs, labels) * (1 - alpha)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWVDrZLGqM48"
   },
   "outputs": [],
   "source": [
    "channels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21] # Frontal = [0, 1, 2, 3, 4, 5], Central = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], Parietal = [18, 19, 20, 21],\n",
    "# All = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "# chan = [[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]]\n",
    "task = 'left' # left, right, foot, tongue\n",
    "apply_filter = True\n",
    "time = [4] #[4, 2]\n",
    "band = [['a', 'a']] #[[0.5, 4], [4, 8], [8, 13], [13, 30], [30, 100], ['a', 'a']]\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 434594,
     "status": "ok",
     "timestamp": 1735678976161,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "SrcX8LrfqM49",
    "outputId": "c63e16ca-7866-4f93-9b8a-dbcd73ae743b"
   },
   "outputs": [],
   "source": [
    "for fl, fh in band:\n",
    "  if fl == 'a':\n",
    "    apply_filter = False\n",
    "  else:\n",
    "    # ------------------------------------------------------------------ Train and Validation Data -------------------------------------------------\n",
    "    fs = 250  # Sampling frequency\n",
    "    order = 5  # Filter order\n",
    "    # Create bandpass filter coefficients\n",
    "    nyq = 0.5 * fs\n",
    "    low = fl / nyq\n",
    "    high = fh / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "  for t in time:\n",
    "    df = []\n",
    "    for i in range(1,10):\n",
    "      data = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000/sub{i}/data_{task}_sub{i}.mat')\n",
    "      data_val = loadmat(f'/gdrive/MyDrive/Motor_Imagery/BCI2a/subjects1000_val/sub{i}/data_{task}_sub{i}.mat')\n",
    "      if t == 4:\n",
    "        data1 = data[f'data_{task}'][:,channels,:]\n",
    "        data_val = data_val[f'data_{task}'][:,channels,:]\n",
    "        data = np.concatenate((data1, data_val), axis=0)\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 2:\n",
    "        data1 = data[f'data_{task}'][:,channels,:500]\n",
    "        data2 = data[f'data_{task}'][:,channels,500:1000]\n",
    "        data1_val = data_val[f'data_{task}'][:,channels,:500]\n",
    "        data2_val = data_val[f'data_{task}'][:,channels,500:1000]\n",
    "        data = np.concatenate((data1, data2, data1_val, data2_val), axis=0)\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "      if t == 1:\n",
    "        data1 = data[f'data_{task}'][:,channels,:250]\n",
    "        data2 = data[f'data_{task}'][:,channels,250:500]\n",
    "        data3 = data[f'data_{task}'][:,channels,500:750]\n",
    "        data4 = data[f'data_{task}'][:,channels,750:1000]\n",
    "        data1_val = data_val[f'data_{task}'][:,channels,:250]\n",
    "        data2_val = data_val[f'data_{task}'][:,channels,250:500]\n",
    "        data3_val = data_val[f'data_{task}'][:,channels,500:750]\n",
    "        data4_val = data_val[f'data_{task}'][:,channels,750:1000]\n",
    "        data = np.concatenate((data1, data2, data3, data4, data1_val, data2_val, data3_val, data4_val), axis=0)\n",
    "        # data = np.concatenate((data1, data2, data3), axis=0) #for data with 75 sample\n",
    "        if apply_filter == True:\n",
    "          data = filtfilt(b, a, data) #frequency filter\n",
    "        label = [i for i in range(1, 10) for _ in range(data.shape[0])]\n",
    "        label = np.array(label).reshape((9, data.shape[0]))\n",
    "        df.append(data)\n",
    "    df = np.array(df)\n",
    "    print(df.shape)\n",
    "    num_trial = df.shape[1]\n",
    "    num_ch = df.shape[2]\n",
    "    num_smaple = df.shape[3]\n",
    "    df = df.reshape((9*num_trial,num_ch,num_smaple))\n",
    "    label = np.array(label)\n",
    "    label = label.reshape((9*num_trial,))\n",
    "    label = label -1\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(df, label, test_size=0.2, random_state=23)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1, random_state=23)\n",
    "    # print(x_train.shape, x_valid.shape, x_test.shape)\n",
    "    # break\n",
    "    x_train = torch.FloatTensor(x_train)\n",
    "    x_train = x_train.unsqueeze(1)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_train = y_train.squeeze()\n",
    "    x_valid = torch.FloatTensor(x_valid)\n",
    "    x_valid = x_valid.unsqueeze(1)\n",
    "    y_valid = torch.LongTensor(y_valid)\n",
    "    y_valid = y_valid.squeeze()\n",
    "    x_test = torch.FloatTensor(x_test)\n",
    "    x_test = x_test.unsqueeze(1)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    y_test = y_test.squeeze()\n",
    "\n",
    "    mu = x_train.mean(dim=0)\n",
    "    std = x_train.std(dim=0)\n",
    "    x_train = (x_train - mu) / std\n",
    "    x_valid = (x_valid - mu) / std\n",
    "    x_test = (x_test - mu) / std\n",
    "\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    valid_dataset = TensorDataset(x_valid, y_valid)\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "    # --------------------------------------------------------------- K-Fold cross-validation -------------------------------------------------------\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    all_loss_test_hist = []\n",
    "    all_acc_test_hist = []\n",
    "    all_precision_test_hist = []\n",
    "    all_recall_test_hist = []\n",
    "    all_f1_test_hist = []\n",
    "    all_loss_test_hist_s = []\n",
    "    all_acc_test_hist_s = []\n",
    "    all_precision_test_hist_s = []\n",
    "    all_recall_test_hist_s = []\n",
    "    all_f1_test_hist_s = []\n",
    "    all_targests_test_hist = []\n",
    "    all_outputs_test_hist = []\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(x_train)):\n",
    "      print(f\"Fold {fold+1}, fl = {fl}, t = {t}\")\n",
    "      train_sampler = SubsetRandomSampler(train_idx)\n",
    "      valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "      train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=32)\n",
    "      valid_loader = DataLoader(train_dataset, sampler=valid_sampler, batch_size=32)\n",
    "      test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "      model = CNN().to(device)\n",
    "      loss_fn = nn.MultiMarginLoss()\n",
    "      lr = 0.00005\n",
    "      wd = 3e-4\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "      best_loss_valid = float('inf')\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        model, loss_train, acc_train = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch)\n",
    "\n",
    "        # Validation\n",
    "        loss_valid, acc_valid, _, _ = validation(model, valid_loader, loss_fn)\n",
    "\n",
    "        if loss_valid < best_loss_valid:\n",
    "            path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "            torch.save(model, path + '/model_5_fold.pt')\n",
    "            best_loss_valid = loss_valid\n",
    "            print('Model Saved!')\n",
    "\n",
    "        print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "        print()\n",
    "\n",
    "      model = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt')\n",
    "      final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(model, test_loader, loss_fn)\n",
    "      acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets_test, all_outputs_test)\n",
    "\n",
    "      all_loss_test_hist.append(final_loss_test)\n",
    "      all_acc_test_hist.append(final_acc_test)\n",
    "      all_precision_test_hist.append(macro_precision)\n",
    "      all_recall_test_hist.append(macro_recall)\n",
    "      all_f1_test_hist.append(macro_f1)\n",
    "\n",
    "      #------------------------------------------------------------KD----------------------------------------------------------------------\n",
    "\n",
    "      teacher = torch.load('/gdrive/MyDrive/Motor_Imagery/model_5_fold.pt')\n",
    "      # teacher.eval()\n",
    "      student = CNN().to(device)\n",
    "      lr = 0.00005\n",
    "      wd = 3e-4\n",
    "      optimizer = optim.Adam(student.parameters(), lr=lr, weight_decay=wd)\n",
    "      loss_fn = nn.MultiMarginLoss()\n",
    "\n",
    "      best_loss_valid_s = torch.inf\n",
    "      epoch_counter = 0\n",
    "\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        student, loss_train, acc_train = train_one_epoch_kd(student,\n",
    "                                                            teacher,\n",
    "                                                            train_loader,\n",
    "                                                            loss_fn_kd,\n",
    "                                                            optimizer,\n",
    "                                                            epoch)\n",
    "        # Validation\n",
    "        loss_valid, acc_valid, _, _ = validation(student,\n",
    "                                                 valid_loader,\n",
    "                                                 loss_fn)\n",
    "\n",
    "        if loss_valid < best_loss_valid_s:\n",
    "          # path = '/gdrive/MyDrive/Motor_Imagery'\n",
    "          # torch.save(model, path + '/model_6_fold.pt')\n",
    "          best_loss_valid_s = loss_valid\n",
    "          print('best')\n",
    "\n",
    "        print(f'Valid: Loss = {loss_valid:.4}, Acc = {acc_valid:.4}')\n",
    "        print()\n",
    "\n",
    "        epoch_counter += 1\n",
    "\n",
    "      # student = torch.load('/gdrive/MyDrive/Motor_Imagery/model_6_fold.pt')\n",
    "      # student.eval()\n",
    "      final_loss_test, final_acc_test, all_targets_test, all_outputs_test = validation(student, test_loader, loss_fn)\n",
    "      acc, macro_precision, macro_recall, macro_f1 = cal_metrics(all_targets_test, all_outputs_test)\n",
    "\n",
    "      all_loss_test_hist_s.append(final_loss_test)\n",
    "      all_acc_test_hist_s.append(final_acc_test)\n",
    "      all_precision_test_hist_s.append(macro_precision)\n",
    "      all_recall_test_hist_s.append(macro_recall)\n",
    "      all_f1_test_hist_s.append(macro_f1)\n",
    "      all_targests_test_hist.append(all_targets_test)\n",
    "      all_outputs_test_hist.append(all_outputs_test)\n",
    "\n",
    "  # --------------------------------------------Save Results----------------------------------------------------------\n",
    "\n",
    "    a1 = sum(all_loss_test_hist)/3\n",
    "    a2 = sum(all_loss_test_hist_s)/3\n",
    "    b1 = sum(all_acc_test_hist)/3\n",
    "    b2 = sum(all_acc_test_hist_s)/3\n",
    "    c1 = sum(all_precision_test_hist)/3\n",
    "    c2 = sum(all_precision_test_hist_s)/3\n",
    "    d1 = sum(all_recall_test_hist)/3\n",
    "    d2 = sum(all_recall_test_hist_s)/3\n",
    "    e1 = sum(all_f1_test_hist)/3\n",
    "    e2 = sum(all_f1_test_hist_s)/3\n",
    "\n",
    "\n",
    "    df = pd.DataFrame([[e1, d1, c1, b1*100, a1, e2, d2, c2, b2*100, a2]],\n",
    "                      columns=['f1', 'recall', 'precision', 'acc', 'loss', 'f1_s', 'recall_s', 'precision_s', 'acc_s', 'loss_s'])\n",
    "\n",
    "    # # Path to the Excel file\n",
    "    # excel_file_path = '/gdrive/MyDrive/Motor_Imagery/resultsyyyyyyyy.xlsx'\n",
    "\n",
    "    # if os.path.exists(excel_file_path):\n",
    "    #     # If the file exists, read the existing data\n",
    "    #     existing_df = pd.read_excel(excel_file_path)\n",
    "\n",
    "    #     # Append the new data\n",
    "    #     updated_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "    # else:\n",
    "    #     # If the file does not exist, create a new DataFrame\n",
    "    #     updated_df = df\n",
    "\n",
    "    # # Write the updated DataFrame back to the Excel file\n",
    "    # with pd.ExcelWriter(excel_file_path, engine='openpyxl', mode='w') as writer:\n",
    "    #     updated_df.to_excel(writer, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcHqRv242O55"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "all_targets_test_hists = np.concatenate([t.cpu().numpy() for t in all_targests_test_hist])\n",
    "all_outputs_test_hists = np.concatenate([t.cpu().numpy() for t in all_outputs_test_hist])\n",
    "\n",
    "# Now you can create the confusion matrix:\n",
    "cm = confusion_matrix(all_targets_test_hists, all_outputs_test_hists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1735679090453,
     "user": {
      "displayName": "m reza",
      "userId": "09528153320198348697"
     },
     "user_tz": 360
    },
    "id": "zqmYDO34-N7_",
    "outputId": "2aa0f6cc-b7c1-4824-b59f-f5e407af1cee"
   },
   "outputs": [],
   "source": [
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  # Normalize confusion matrix\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(9)\n",
    "plt.xticks(tick_marks, ['1','2','3','4','5','6','7','8','9'], rotation=45)\n",
    "plt.yticks(tick_marks, ['1','2','3','4','5','6','7','8','9'])\n",
    "fmt = '.2f'\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "   plt.text(j, i, format(cm[i, j], fmt), ha='center', va='center',\n",
    "            color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(save_path, format='png')\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qL0K8Z91OMxx",
    "ELwgzDfGFXvr",
    "fQFlVvECeZsD",
    "mbpEEA0g1-Be",
    "DY5JMY1Yftsr",
    "G9DiYhr9Yj-p",
    "uawPxD41yGuK",
    "FpY1h4DZvHmc",
    "3aQRTc6n7-hi",
    "7v0bwJgZ8mvX",
    "B_LljZFVAFPA",
    "qfQWm6fDkAtE",
    "lpJ3wtyctQJH",
    "BrHQCv7q7LF_",
    "XuNumM_aqCI2",
    "PIO2T27yp9p3",
    "IxzGERqwxG58",
    "NbPIXZeQCIT8",
    "fdCZLlEtCBkG",
    "9csGqnCvtFAS",
    "mAQiLjuLwtPY",
    "bm2XCErxjVlO",
    "oK20iNRI3Xxb",
    "z5zctW6jy9Lk",
    "EwnX3SE4y9Mp",
    "eQwnmMARy9NF",
    "Qmf-oSeuy9NJ",
    "4En2sFilB46f"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
